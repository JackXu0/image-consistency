{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path, image_count, fake_exif_labels): \n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.image_count = image_count\n",
    "        self.transforms = transforms.Compose([\n",
    "         \ttransforms.CenterCrop(100),\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.dataset_path+str(index)+\".jpg\").convert('RGB')\n",
    "        \n",
    "        return self.transforms(image), fake_exif_labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "temperature = 0.5\n",
    "training_images_count = 10\n",
    "\n",
    "fake_exif_labels = torch.rand(training_images_count, 10)\n",
    "\n",
    "\n",
    "train_data = CustomDataset('dataset/cvFinalData/train/', training_images_count, fake_exif_labels)\n",
    "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EXIF(nn.Module):\n",
    "    def __init__(self, encoder, n_features, projection_dim):\n",
    "        super(EXIF, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.n_features, 300, bias=False),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(300, projection_dim, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.projector(x)\n",
    "        x = F.normalize(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torchvision.models.resnet50(pretrained=False)\n",
    "n_features = 1000\n",
    "projection_dim = 10\n",
    "device = 'cpu'\n",
    "model = EXIF(encoder, n_features, projection_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training  10 / 10  Loss:  tensor(2.8077, grad_fn=<DivBackward0>)\n",
      "Epoch:  1\n",
      "Training  10 / 10  Loss:  tensor(2.0298, grad_fn=<DivBackward0>)\n",
      "Epoch:  2\n",
      "Training  10 / 10  Loss:  tensor(1.5618, grad_fn=<DivBackward0>)\n",
      "Epoch:  3\n",
      "Training  10 / 10  Loss:  tensor(1.5511, grad_fn=<DivBackward0>)\n",
      "Epoch:  4\n",
      "Training  10 / 10  Loss:  tensor(1.5533, grad_fn=<DivBackward0>)\n",
      "Epoch:  5\n",
      "Training  10 / 10  Loss:  tensor(1.5325, grad_fn=<DivBackward0>)\n",
      "Epoch:  6\n",
      "Training  10 / 10  Loss:  tensor(1.5273, grad_fn=<DivBackward0>)\n",
      "Epoch:  7\n",
      "Training  10 / 10  Loss:  tensor(1.5563, grad_fn=<DivBackward0>)\n",
      "Epoch:  8\n",
      "Training  10 / 10  Loss:  tensor(1.5117, grad_fn=<DivBackward0>)\n",
      "Epoch:  9\n",
      "Training  10 / 10  Loss:  tensor(1.5029, grad_fn=<DivBackward0>)\n",
      "Epoch:  10\n",
      "Training  10 / 10  Loss:  tensor(1.5305, grad_fn=<DivBackward0>)\n",
      "Epoch:  11\n",
      "Training  10 / 10  Loss:  tensor(1.5051, grad_fn=<DivBackward0>)\n",
      "Epoch:  12\n",
      "Training  10 / 10  Loss:  tensor(1.5154, grad_fn=<DivBackward0>)\n",
      "Epoch:  13\n",
      "Training  10 / 10  Loss:  tensor(1.5501, grad_fn=<DivBackward0>)\n",
      "Epoch:  14\n",
      "Training  10 / 10  Loss:  tensor(1.4877, grad_fn=<DivBackward0>)\n",
      "Epoch:  15\n",
      "Training  10 / 10  Loss:  tensor(1.4971, grad_fn=<DivBackward0>)\n",
      "Epoch:  16\n",
      "Training  10 / 10  Loss:  tensor(1.4964, grad_fn=<DivBackward0>)\n",
      "Epoch:  17\n",
      "Training  10 / 10  Loss:  tensor(1.4950, grad_fn=<DivBackward0>)\n",
      "Epoch:  18\n",
      "Training  10 / 10  Loss:  tensor(1.4958, grad_fn=<DivBackward0>)\n",
      "Epoch:  19\n",
      "Training  10 / 10  Loss:  tensor(1.5282, grad_fn=<DivBackward0>)\n",
      "Epoch:  20\n",
      "Training  10 / 10  Loss:  tensor(1.4897, grad_fn=<DivBackward0>)\n",
      "Epoch:  21\n",
      "Training  10 / 10  Loss:  tensor(1.4732, grad_fn=<DivBackward0>)\n",
      "Epoch:  22\n",
      "Training  10 / 10  Loss:  tensor(1.3766, grad_fn=<DivBackward0>)\n",
      "Epoch:  23\n",
      "Training  10 / 10  Loss:  tensor(1.2461, grad_fn=<DivBackward0>)\n",
      "Epoch:  24\n",
      "Training  10 / 10  Loss:  tensor(1.2643, grad_fn=<DivBackward0>)\n",
      "Epoch:  25\n",
      "Training  10 / 10  Loss:  tensor(1.2568, grad_fn=<DivBackward0>)\n",
      "Epoch:  26\n",
      "Training  10 / 10  Loss:  tensor(1.2313, grad_fn=<DivBackward0>)\n",
      "Epoch:  27\n",
      "Training  10 / 10  Loss:  tensor(1.2793, grad_fn=<DivBackward0>)\n",
      "Epoch:  28\n",
      "Training  10 / 10  Loss:  tensor(1.2330, grad_fn=<DivBackward0>)\n",
      "Epoch:  29\n",
      "Training  10 / 10  Loss:  tensor(1.2349, grad_fn=<DivBackward0>)\n",
      "Epoch:  30\n",
      "Training  10 / 10  Loss:  tensor(1.2726, grad_fn=<DivBackward0>)\n",
      "Epoch:  31\n",
      "Training  10 / 10  Loss:  tensor(1.2536, grad_fn=<DivBackward0>)\n",
      "Epoch:  32\n",
      "Training  10 / 10  Loss:  tensor(1.2182, grad_fn=<DivBackward0>)\n",
      "Epoch:  33\n",
      "Training  10 / 10  Loss:  tensor(1.1996, grad_fn=<DivBackward0>)\n",
      "Epoch:  34\n",
      "Training  10 / 10  Loss:  tensor(1.2292, grad_fn=<DivBackward0>)\n",
      "Epoch:  35\n",
      "Training  10 / 10  Loss:  tensor(1.2276, grad_fn=<DivBackward0>)\n",
      "Epoch:  36\n",
      "Training  10 / 10  Loss:  tensor(1.2402, grad_fn=<DivBackward0>)\n",
      "Epoch:  37\n",
      "Training  10 / 10  Loss:  tensor(1.2138, grad_fn=<DivBackward0>)\n",
      "Epoch:  38\n",
      "Training  10 / 10  Loss:  tensor(1.1950, grad_fn=<DivBackward0>)\n",
      "Epoch:  39\n",
      "Training  10 / 10  Loss:  tensor(1.1782, grad_fn=<DivBackward0>)\n",
      "Epoch:  40\n",
      "Training  10 / 10  Loss:  tensor(1.1693, grad_fn=<DivBackward0>)\n",
      "Epoch:  41\n",
      "Training  10 / 10  Loss:  tensor(1.1832, grad_fn=<DivBackward0>)\n",
      "Epoch:  42\n",
      "Training  10 / 10  Loss:  tensor(1.1335, grad_fn=<DivBackward0>)\n",
      "Epoch:  43\n",
      "Training  10 / 10  Loss:  tensor(1.1293, grad_fn=<DivBackward0>)\n",
      "Epoch:  44\n",
      "Training  10 / 10  Loss:  tensor(1.1412, grad_fn=<DivBackward0>)\n",
      "Epoch:  45\n",
      "Training  10 / 10  Loss:  tensor(1.1287, grad_fn=<DivBackward0>)\n",
      "Epoch:  46\n",
      "Training  10 / 10  Loss:  tensor(1.1454, grad_fn=<DivBackward0>)\n",
      "Epoch:  47\n",
      "Training  10 / 10  Loss:  tensor(1.1357, grad_fn=<DivBackward0>)\n",
      "Epoch:  48\n",
      "Training  10 / 10  Loss:  tensor(1.1383, grad_fn=<DivBackward0>)\n",
      "Epoch:  49\n",
      "Training  10 / 10  Loss:  tensor(1.1449, grad_fn=<DivBackward0>)\n",
      "Epoch:  50\n",
      "Training  10 / 10  Loss:  tensor(1.1233, grad_fn=<DivBackward0>)\n",
      "Epoch:  51\n",
      "Training  10 / 10  Loss:  tensor(1.1377, grad_fn=<DivBackward0>)\n",
      "Epoch:  52\n",
      "Training  10 / 10  Loss:  tensor(1.1366, grad_fn=<DivBackward0>)\n",
      "Epoch:  53\n",
      "Training  10 / 10  Loss:  tensor(1.1458, grad_fn=<DivBackward0>)\n",
      "Epoch:  54\n",
      "Training  10 / 10  Loss:  tensor(1.1328, grad_fn=<DivBackward0>)\n",
      "Epoch:  55\n",
      "Training  10 / 10  Loss:  tensor(1.1146, grad_fn=<DivBackward0>)\n",
      "Epoch:  56\n",
      "Training  10 / 10  Loss:  tensor(1.1108, grad_fn=<DivBackward0>)\n",
      "Epoch:  57\n",
      "Training  10 / 10  Loss:  tensor(1.1382, grad_fn=<DivBackward0>)\n",
      "Epoch:  58\n",
      "Training  10 / 10  Loss:  tensor(1.1231, grad_fn=<DivBackward0>)\n",
      "Epoch:  59\n",
      "Training  10 / 10  Loss:  tensor(1.1367, grad_fn=<DivBackward0>)\n",
      "Epoch:  60\n",
      "Training  10 / 10  Loss:  tensor(1.1059, grad_fn=<DivBackward0>)\n",
      "Epoch:  61\n",
      "Training  10 / 10  Loss:  tensor(1.1045, grad_fn=<DivBackward0>)\n",
      "Epoch:  62\n",
      "Training  10 / 10  Loss:  tensor(1.0989, grad_fn=<DivBackward0>)\n",
      "Epoch:  63\n",
      "Training  10 / 10  Loss:  tensor(1.1010, grad_fn=<DivBackward0>)\n",
      "Epoch:  64\n",
      "Training  10 / 10  Loss:  tensor(1.1347, grad_fn=<DivBackward0>)\n",
      "Epoch:  65\n",
      "Training  10 / 10  Loss:  tensor(1.0924, grad_fn=<DivBackward0>)\n",
      "Epoch:  66\n",
      "Training  10 / 10  Loss:  tensor(1.1169, grad_fn=<DivBackward0>)\n",
      "Epoch:  67\n",
      "Training  10 / 10  Loss:  tensor(1.1166, grad_fn=<DivBackward0>)\n",
      "Epoch:  68\n",
      "Training  10 / 10  Loss:  tensor(1.1083, grad_fn=<DivBackward0>)\n",
      "Epoch:  69\n",
      "Training  10 / 10  Loss:  tensor(1.1164, grad_fn=<DivBackward0>)\n",
      "Epoch:  70\n",
      "Training  10 / 10  Loss:  tensor(1.0959, grad_fn=<DivBackward0>)\n",
      "Epoch:  71\n",
      "Training  10 / 10  Loss:  tensor(1.1209, grad_fn=<DivBackward0>)\n",
      "Epoch:  72\n",
      "Training  10 / 10  Loss:  tensor(1.0957, grad_fn=<DivBackward0>)\n",
      "Epoch:  73\n",
      "Training  10 / 10  Loss:  tensor(1.1181, grad_fn=<DivBackward0>)\n",
      "Epoch:  74\n",
      "Training  10 / 10  Loss:  tensor(1.0923, grad_fn=<DivBackward0>)\n",
      "Epoch:  75\n",
      "Training  10 / 10  Loss:  tensor(1.0953, grad_fn=<DivBackward0>)\n",
      "Epoch:  76\n",
      "Training  10 / 10  Loss:  tensor(1.1287, grad_fn=<DivBackward0>)\n",
      "Epoch:  77\n",
      "Training  10 / 10  Loss:  tensor(1.1253, grad_fn=<DivBackward0>)\n",
      "Epoch:  78\n",
      "Training  10 / 10  Loss:  tensor(1.1281, grad_fn=<DivBackward0>)\n",
      "Epoch:  79\n",
      "Training  10 / 10  Loss:  tensor(1.1216, grad_fn=<DivBackward0>)\n",
      "Epoch:  80\n",
      "Training  10 / 10  Loss:  tensor(1.1280, grad_fn=<DivBackward0>)\n",
      "Epoch:  81\n",
      "Training  10 / 10  Loss:  tensor(1.1249, grad_fn=<DivBackward0>)\n",
      "Epoch:  82\n",
      "Training  10 / 10  Loss:  tensor(1.1171, grad_fn=<DivBackward0>)\n",
      "Epoch:  83\n",
      "Training  10 / 10  Loss:  tensor(1.0815, grad_fn=<DivBackward0>)\n",
      "Epoch:  84\n",
      "Training  10 / 10  Loss:  tensor(1.0704, grad_fn=<DivBackward0>)\n",
      "Epoch:  85\n",
      "Training  10 / 10  Loss:  tensor(1.1635, grad_fn=<DivBackward0>)\n",
      "Epoch:  86\n",
      "Training  10 / 10  Loss:  tensor(1.1027, grad_fn=<DivBackward0>)\n",
      "Epoch:  87\n",
      "Training  10 / 10  Loss:  tensor(1.0877, grad_fn=<DivBackward0>)\n",
      "Epoch:  88\n",
      "Training  10 / 10  Loss:  tensor(1.0704, grad_fn=<DivBackward0>)\n",
      "Epoch:  89\n",
      "Training  10 / 10  Loss:  tensor(1.0835, grad_fn=<DivBackward0>)\n",
      "Epoch:  90\n",
      "Training  10 / 10  Loss:  tensor(1.0864, grad_fn=<DivBackward0>)\n",
      "Epoch:  91\n",
      "Training  10 / 10  Loss:  tensor(1.1031, grad_fn=<DivBackward0>)\n",
      "Epoch:  92\n",
      "Training  10 / 10  Loss:  tensor(1.0630, grad_fn=<DivBackward0>)\n",
      "Epoch:  93\n",
      "Training  10 / 10  Loss:  tensor(1.0801, grad_fn=<DivBackward0>)\n",
      "Epoch:  94\n",
      "Training  10 / 10  Loss:  tensor(1.0809, grad_fn=<DivBackward0>)\n",
      "Epoch:  95\n",
      "Training  10 / 10  Loss:  tensor(1.0911, grad_fn=<DivBackward0>)\n",
      "Epoch:  96\n",
      "Training  10 / 10  Loss:  tensor(1.0647, grad_fn=<DivBackward0>)\n",
      "Epoch:  97\n",
      "Training  10 / 10  Loss:  tensor(1.0906, grad_fn=<DivBackward0>)\n",
      "Epoch:  98\n",
      "Training  10 / 10  Loss:  tensor(1.0675, grad_fn=<DivBackward0>)\n",
      "Epoch:  99\n",
      "Training  10 / 10  Loss:  tensor(1.0874, grad_fn=<DivBackward0>)\n",
      "Epoch:  100\n",
      "Training  10 / 10  Loss:  tensor(1.0708, grad_fn=<DivBackward0>)\n",
      "Epoch:  101\n",
      "Training  10 / 10  Loss:  tensor(1.0635, grad_fn=<DivBackward0>)\n",
      "Epoch:  102\n",
      "Training  10 / 10  Loss:  tensor(1.0760, grad_fn=<DivBackward0>)\n",
      "Epoch:  103\n",
      "Training  10 / 10  Loss:  tensor(1.0706, grad_fn=<DivBackward0>)\n",
      "Epoch:  104\n",
      "Training  10 / 10  Loss:  tensor(1.0860, grad_fn=<DivBackward0>)\n",
      "Epoch:  105\n",
      "Training  10 / 10  Loss:  tensor(1.0842, grad_fn=<DivBackward0>)\n",
      "Epoch:  106\n",
      "Training  10 / 10  Loss:  tensor(1.0537, grad_fn=<DivBackward0>)\n",
      "Epoch:  107\n",
      "Training  10 / 10  Loss:  tensor(1.0653, grad_fn=<DivBackward0>)\n",
      "Epoch:  108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(1.0812, grad_fn=<DivBackward0>)\n",
      "Epoch:  109\n",
      "Training  10 / 10  Loss:  tensor(1.0804, grad_fn=<DivBackward0>)\n",
      "Epoch:  110\n",
      "Training  10 / 10  Loss:  tensor(1.0659, grad_fn=<DivBackward0>)\n",
      "Epoch:  111\n",
      "Training  10 / 10  Loss:  tensor(1.0524, grad_fn=<DivBackward0>)\n",
      "Epoch:  112\n",
      "Training  10 / 10  Loss:  tensor(1.1029, grad_fn=<DivBackward0>)\n",
      "Epoch:  113\n",
      "Training  10 / 10  Loss:  tensor(1.0828, grad_fn=<DivBackward0>)\n",
      "Epoch:  114\n",
      "Training  10 / 10  Loss:  tensor(1.0761, grad_fn=<DivBackward0>)\n",
      "Epoch:  115\n",
      "Training  10 / 10  Loss:  tensor(1.1035, grad_fn=<DivBackward0>)\n",
      "Epoch:  116\n",
      "Training  10 / 10  Loss:  tensor(1.1303, grad_fn=<DivBackward0>)\n",
      "Epoch:  117\n",
      "Training  10 / 10  Loss:  tensor(1.0986, grad_fn=<DivBackward0>)\n",
      "Epoch:  118\n",
      "Training  10 / 10  Loss:  tensor(1.0935, grad_fn=<DivBackward0>)\n",
      "Epoch:  119\n",
      "Training  10 / 10  Loss:  tensor(1.0778, grad_fn=<DivBackward0>)\n",
      "Epoch:  120\n",
      "Training  10 / 10  Loss:  tensor(1.1430, grad_fn=<DivBackward0>)\n",
      "Epoch:  121\n",
      "Training  10 / 10  Loss:  tensor(1.1069, grad_fn=<DivBackward0>)\n",
      "Epoch:  122\n",
      "Training  10 / 10  Loss:  tensor(1.0741, grad_fn=<DivBackward0>)\n",
      "Epoch:  123\n",
      "Training  10 / 10  Loss:  tensor(1.0966, grad_fn=<DivBackward0>)\n",
      "Epoch:  124\n",
      "Training  10 / 10  Loss:  tensor(1.0981, grad_fn=<DivBackward0>)\n",
      "Epoch:  125\n",
      "Training  10 / 10  Loss:  tensor(1.0941, grad_fn=<DivBackward0>)\n",
      "Epoch:  126\n",
      "Training  10 / 10  Loss:  tensor(1.0735, grad_fn=<DivBackward0>)\n",
      "Epoch:  127\n",
      "Training  10 / 10  Loss:  tensor(1.0953, grad_fn=<DivBackward0>)\n",
      "Epoch:  128\n",
      "Training  10 / 10  Loss:  tensor(1.0688, grad_fn=<DivBackward0>)\n",
      "Epoch:  129\n",
      "Training  10 / 10  Loss:  tensor(1.1078, grad_fn=<DivBackward0>)\n",
      "Epoch:  130\n",
      "Training  10 / 10  Loss:  tensor(1.0709, grad_fn=<DivBackward0>)\n",
      "Epoch:  131\n",
      "Training  10 / 10  Loss:  tensor(1.0839, grad_fn=<DivBackward0>)\n",
      "Epoch:  132\n",
      "Training  10 / 10  Loss:  tensor(1.0780, grad_fn=<DivBackward0>)\n",
      "Epoch:  133\n",
      "Training  10 / 10  Loss:  tensor(1.0626, grad_fn=<DivBackward0>)\n",
      "Epoch:  134\n",
      "Training  10 / 10  Loss:  tensor(1.0597, grad_fn=<DivBackward0>)\n",
      "Epoch:  135\n",
      "Training  10 / 10  Loss:  tensor(1.0598, grad_fn=<DivBackward0>)\n",
      "Epoch:  136\n",
      "Training  10 / 10  Loss:  tensor(1.0544, grad_fn=<DivBackward0>)\n",
      "Epoch:  137\n",
      "Training  10 / 10  Loss:  tensor(1.0404, grad_fn=<DivBackward0>)\n",
      "Epoch:  138\n",
      "Training  10 / 10  Loss:  tensor(1.0709, grad_fn=<DivBackward0>)\n",
      "Epoch:  139\n",
      "Training  10 / 10  Loss:  tensor(1.0405, grad_fn=<DivBackward0>)\n",
      "Epoch:  140\n",
      "Training  10 / 10  Loss:  tensor(1.0794, grad_fn=<DivBackward0>)\n",
      "Epoch:  141\n",
      "Training  10 / 10  Loss:  tensor(1.0846, grad_fn=<DivBackward0>)\n",
      "Epoch:  142\n",
      "Training  10 / 10  Loss:  tensor(1.0823, grad_fn=<DivBackward0>)\n",
      "Epoch:  143\n",
      "Training  10 / 10  Loss:  tensor(1.0647, grad_fn=<DivBackward0>)\n",
      "Epoch:  144\n",
      "Training  10 / 10  Loss:  tensor(1.0794, grad_fn=<DivBackward0>)\n",
      "Epoch:  145\n",
      "Training  10 / 10  Loss:  tensor(1.0753, grad_fn=<DivBackward0>)\n",
      "Epoch:  146\n",
      "Training  10 / 10  Loss:  tensor(1.0961, grad_fn=<DivBackward0>)\n",
      "Epoch:  147\n",
      "Training  10 / 10  Loss:  tensor(1.0864, grad_fn=<DivBackward0>)\n",
      "Epoch:  148\n",
      "Training  10 / 10  Loss:  tensor(1.1010, grad_fn=<DivBackward0>)\n",
      "Epoch:  149\n",
      "Training  10 / 10  Loss:  tensor(1.0495, grad_fn=<DivBackward0>)\n",
      "Epoch:  150\n",
      "Training  10 / 10  Loss:  tensor(1.0745, grad_fn=<DivBackward0>)\n",
      "Epoch:  151\n",
      "Training  10 / 10  Loss:  tensor(1.0659, grad_fn=<DivBackward0>)\n",
      "Epoch:  152\n",
      "Training  10 / 10  Loss:  tensor(1.0801, grad_fn=<DivBackward0>)\n",
      "Epoch:  153\n",
      "Training  10 / 10  Loss:  tensor(1.0443, grad_fn=<DivBackward0>)\n",
      "Epoch:  154\n",
      "Training  10 / 10  Loss:  tensor(1.0854, grad_fn=<DivBackward0>)\n",
      "Epoch:  155\n",
      "Training  10 / 10  Loss:  tensor(1.0608, grad_fn=<DivBackward0>)\n",
      "Epoch:  156\n",
      "Training  10 / 10  Loss:  tensor(1.0839, grad_fn=<DivBackward0>)\n",
      "Epoch:  157\n",
      "Training  10 / 10  Loss:  tensor(1.0592, grad_fn=<DivBackward0>)\n",
      "Epoch:  158\n",
      "Training  10 / 10  Loss:  tensor(1.0588, grad_fn=<DivBackward0>)\n",
      "Epoch:  159\n",
      "Training  10 / 10  Loss:  tensor(1.0533, grad_fn=<DivBackward0>)\n",
      "Epoch:  160\n",
      "Training  10 / 10  Loss:  tensor(1.0257, grad_fn=<DivBackward0>)\n",
      "Epoch:  161\n",
      "Training  10 / 10  Loss:  tensor(1.0440, grad_fn=<DivBackward0>)\n",
      "Epoch:  162\n",
      "Training  10 / 10  Loss:  tensor(1.0605, grad_fn=<DivBackward0>)\n",
      "Epoch:  163\n",
      "Training  10 / 10  Loss:  tensor(1.0449, grad_fn=<DivBackward0>)\n",
      "Epoch:  164\n",
      "Training  10 / 10  Loss:  tensor(1.0383, grad_fn=<DivBackward0>)\n",
      "Epoch:  165\n",
      "Training  10 / 10  Loss:  tensor(1.0733, grad_fn=<DivBackward0>)\n",
      "Epoch:  166\n",
      "Training  10 / 10  Loss:  tensor(1.0914, grad_fn=<DivBackward0>)\n",
      "Epoch:  167\n",
      "Training  10 / 10  Loss:  tensor(1.0643, grad_fn=<DivBackward0>)\n",
      "Epoch:  168\n",
      "Training  10 / 10  Loss:  tensor(1.0629, grad_fn=<DivBackward0>)\n",
      "Epoch:  169\n",
      "Training  10 / 10  Loss:  tensor(1.0931, grad_fn=<DivBackward0>)\n",
      "Epoch:  170\n",
      "Training  10 / 10  Loss:  tensor(1.0868, grad_fn=<DivBackward0>)\n",
      "Epoch:  171\n",
      "Training  10 / 10  Loss:  tensor(1.0525, grad_fn=<DivBackward0>)\n",
      "Epoch:  172\n",
      "Training  10 / 10  Loss:  tensor(1.0824, grad_fn=<DivBackward0>)\n",
      "Epoch:  173\n",
      "Training  10 / 10  Loss:  tensor(1.0540, grad_fn=<DivBackward0>)\n",
      "Epoch:  174\n",
      "Training  10 / 10  Loss:  tensor(1.0536, grad_fn=<DivBackward0>)\n",
      "Epoch:  175\n",
      "Training  10 / 10  Loss:  tensor(1.0360, grad_fn=<DivBackward0>)\n",
      "Epoch:  176\n",
      "Training  10 / 10  Loss:  tensor(1.0615, grad_fn=<DivBackward0>)\n",
      "Epoch:  177\n",
      "Training  10 / 10  Loss:  tensor(1.0543, grad_fn=<DivBackward0>)\n",
      "Epoch:  178\n",
      "Training  10 / 10  Loss:  tensor(1.0649, grad_fn=<DivBackward0>)\n",
      "Epoch:  179\n",
      "Training  10 / 10  Loss:  tensor(1.0761, grad_fn=<DivBackward0>)\n",
      "Epoch:  180\n",
      "Training  10 / 10  Loss:  tensor(1.0738, grad_fn=<DivBackward0>)\n",
      "Epoch:  181\n",
      "Training  10 / 10  Loss:  tensor(1.0514, grad_fn=<DivBackward0>)\n",
      "Epoch:  182\n",
      "Training  10 / 10  Loss:  tensor(1.0593, grad_fn=<DivBackward0>)\n",
      "Epoch:  183\n",
      "Training  10 / 10  Loss:  tensor(1.0531, grad_fn=<DivBackward0>)\n",
      "Epoch:  184\n",
      "Training  10 / 10  Loss:  tensor(1.0503, grad_fn=<DivBackward0>)\n",
      "Epoch:  185\n",
      "Training  10 / 10  Loss:  tensor(1.0825, grad_fn=<DivBackward0>)\n",
      "Epoch:  186\n",
      "Training  10 / 10  Loss:  tensor(1.0667, grad_fn=<DivBackward0>)\n",
      "Epoch:  187\n",
      "Training  10 / 10  Loss:  tensor(1.0868, grad_fn=<DivBackward0>)\n",
      "Epoch:  188\n",
      "Training  10 / 10  Loss:  tensor(1.0836, grad_fn=<DivBackward0>)\n",
      "Epoch:  189\n",
      "Training  10 / 10  Loss:  tensor(1.0907, grad_fn=<DivBackward0>)\n",
      "Epoch:  190\n",
      "Training  10 / 10  Loss:  tensor(1.0929, grad_fn=<DivBackward0>)\n",
      "Epoch:  191\n",
      "Training  10 / 10  Loss:  tensor(1.0716, grad_fn=<DivBackward0>)\n",
      "Epoch:  192\n",
      "Training  10 / 10  Loss:  tensor(1.0777, grad_fn=<DivBackward0>)\n",
      "Epoch:  193\n",
      "Training  10 / 10  Loss:  tensor(1.0901, grad_fn=<DivBackward0>)\n",
      "Epoch:  194\n",
      "Training  10 / 10  Loss:  tensor(1.0489, grad_fn=<DivBackward0>)\n",
      "Epoch:  195\n",
      "Training  10 / 10  Loss:  tensor(1.0732, grad_fn=<DivBackward0>)\n",
      "Epoch:  196\n",
      "Training  10 / 10  Loss:  tensor(1.0402, grad_fn=<DivBackward0>)\n",
      "Epoch:  197\n",
      "Training  10 / 10  Loss:  tensor(1.0523, grad_fn=<DivBackward0>)\n",
      "Epoch:  198\n",
      "Training  10 / 10  Loss:  tensor(1.0656, grad_fn=<DivBackward0>)\n",
      "Epoch:  199\n",
      "Training  10 / 10  Loss:  tensor(1.0445, grad_fn=<DivBackward0>)\n",
      "Epoch:  200\n",
      "Training  10 / 10  Loss:  tensor(1.0675, grad_fn=<DivBackward0>)\n",
      "Epoch:  201\n",
      "Training  10 / 10  Loss:  tensor(1.0302, grad_fn=<DivBackward0>)\n",
      "Epoch:  202\n",
      "Training  10 / 10  Loss:  tensor(1.0658, grad_fn=<DivBackward0>)\n",
      "Epoch:  203\n",
      "Training  10 / 10  Loss:  tensor(1.0560, grad_fn=<DivBackward0>)\n",
      "Epoch:  204\n",
      "Training  10 / 10  Loss:  tensor(1.0786, grad_fn=<DivBackward0>)\n",
      "Epoch:  205\n",
      "Training  10 / 10  Loss:  tensor(1.0376, grad_fn=<DivBackward0>)\n",
      "Epoch:  206\n",
      "Training  10 / 10  Loss:  tensor(1.0612, grad_fn=<DivBackward0>)\n",
      "Epoch:  207\n",
      "Training  10 / 10  Loss:  tensor(1.0561, grad_fn=<DivBackward0>)\n",
      "Epoch:  208\n",
      "Training  10 / 10  Loss:  tensor(1.0680, grad_fn=<DivBackward0>)\n",
      "Epoch:  209\n",
      "Training  10 / 10  Loss:  tensor(1.0291, grad_fn=<DivBackward0>)\n",
      "Epoch:  210\n",
      "Training  10 / 10  Loss:  tensor(1.0516, grad_fn=<DivBackward0>)\n",
      "Epoch:  211\n",
      "Training  10 / 10  Loss:  tensor(1.0454, grad_fn=<DivBackward0>)\n",
      "Epoch:  212\n",
      "Training  10 / 10  Loss:  tensor(1.0671, grad_fn=<DivBackward0>)\n",
      "Epoch:  213\n",
      "Training  10 / 10  Loss:  tensor(1.0975, grad_fn=<DivBackward0>)\n",
      "Epoch:  214\n",
      "Training  10 / 10  Loss:  tensor(1.0429, grad_fn=<DivBackward0>)\n",
      "Epoch:  215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(1.0425, grad_fn=<DivBackward0>)\n",
      "Epoch:  216\n",
      "Training  10 / 10  Loss:  tensor(1.0504, grad_fn=<DivBackward0>)\n",
      "Epoch:  217\n",
      "Training  10 / 10  Loss:  tensor(1.0550, grad_fn=<DivBackward0>)\n",
      "Epoch:  218\n",
      "Training  10 / 10  Loss:  tensor(1.0487, grad_fn=<DivBackward0>)\n",
      "Epoch:  219\n",
      "Training  10 / 10  Loss:  tensor(1.0287, grad_fn=<DivBackward0>)\n",
      "Epoch:  220\n",
      "Training  10 / 10  Loss:  tensor(1.0624, grad_fn=<DivBackward0>)\n",
      "Epoch:  221\n",
      "Training  10 / 10  Loss:  tensor(1.0337, grad_fn=<DivBackward0>)\n",
      "Epoch:  222\n",
      "Training  10 / 10  Loss:  tensor(1.0376, grad_fn=<DivBackward0>)\n",
      "Epoch:  223\n",
      "Training  10 / 10  Loss:  tensor(1.0360, grad_fn=<DivBackward0>)\n",
      "Epoch:  224\n",
      "Training  10 / 10  Loss:  tensor(1.0167, grad_fn=<DivBackward0>)\n",
      "Epoch:  225\n",
      "Training  10 / 10  Loss:  tensor(1.0496, grad_fn=<DivBackward0>)\n",
      "Epoch:  226\n",
      "Training  10 / 10  Loss:  tensor(1.0477, grad_fn=<DivBackward0>)\n",
      "Epoch:  227\n",
      "Training  10 / 10  Loss:  tensor(1.0756, grad_fn=<DivBackward0>)\n",
      "Epoch:  228\n",
      "Training  10 / 10  Loss:  tensor(1.0436, grad_fn=<DivBackward0>)\n",
      "Epoch:  229\n",
      "Training  10 / 10  Loss:  tensor(1.0694, grad_fn=<DivBackward0>)\n",
      "Epoch:  230\n",
      "Training  10 / 10  Loss:  tensor(1.0269, grad_fn=<DivBackward0>)\n",
      "Epoch:  231\n",
      "Training  10 / 10  Loss:  tensor(1.0263, grad_fn=<DivBackward0>)\n",
      "Epoch:  232\n",
      "Training  10 / 10  Loss:  tensor(1.0611, grad_fn=<DivBackward0>)\n",
      "Epoch:  233\n",
      "Training  10 / 10  Loss:  tensor(1.0697, grad_fn=<DivBackward0>)\n",
      "Epoch:  234\n",
      "Training  10 / 10  Loss:  tensor(1.0135, grad_fn=<DivBackward0>)\n",
      "Epoch:  235\n",
      "Training  10 / 10  Loss:  tensor(1.0368, grad_fn=<DivBackward0>)\n",
      "Epoch:  236\n",
      "Training  10 / 10  Loss:  tensor(1.0184, grad_fn=<DivBackward0>)\n",
      "Epoch:  237\n",
      "Training  10 / 10  Loss:  tensor(1.0677, grad_fn=<DivBackward0>)\n",
      "Epoch:  238\n",
      "Training  10 / 10  Loss:  tensor(1.0466, grad_fn=<DivBackward0>)\n",
      "Epoch:  239\n",
      "Training  10 / 10  Loss:  tensor(1.0139, grad_fn=<DivBackward0>)\n",
      "Epoch:  240\n",
      "Training  10 / 10  Loss:  tensor(1.0121, grad_fn=<DivBackward0>)\n",
      "Epoch:  241\n",
      "Training  10 / 10  Loss:  tensor(1.0312, grad_fn=<DivBackward0>)\n",
      "Epoch:  242\n",
      "Training  10 / 10  Loss:  tensor(1.0161, grad_fn=<DivBackward0>)\n",
      "Epoch:  243\n",
      "Training  10 / 10  Loss:  tensor(1.0288, grad_fn=<DivBackward0>)\n",
      "Epoch:  244\n",
      "Training  10 / 10  Loss:  tensor(1.0174, grad_fn=<DivBackward0>)\n",
      "Epoch:  245\n",
      "Training  10 / 10  Loss:  tensor(1.0097, grad_fn=<DivBackward0>)\n",
      "Epoch:  246\n",
      "Training  10 / 10  Loss:  tensor(1.0594, grad_fn=<DivBackward0>)\n",
      "Epoch:  247\n",
      "Training  10 / 10  Loss:  tensor(1.0405, grad_fn=<DivBackward0>)\n",
      "Epoch:  248\n",
      "Training  10 / 10  Loss:  tensor(1.0759, grad_fn=<DivBackward0>)\n",
      "Epoch:  249\n",
      "Training  10 / 10  Loss:  tensor(1.0518, grad_fn=<DivBackward0>)\n",
      "Epoch:  250\n",
      "Training  10 / 10  Loss:  tensor(1.0261, grad_fn=<DivBackward0>)\n",
      "Epoch:  251\n",
      "Training  10 / 10  Loss:  tensor(1.0096, grad_fn=<DivBackward0>)\n",
      "Epoch:  252\n",
      "Training  10 / 10  Loss:  tensor(1.0184, grad_fn=<DivBackward0>)\n",
      "Epoch:  253\n",
      "Training  10 / 10  Loss:  tensor(1.0407, grad_fn=<DivBackward0>)\n",
      "Epoch:  254\n",
      "Training  10 / 10  Loss:  tensor(1.0349, grad_fn=<DivBackward0>)\n",
      "Epoch:  255\n",
      "Training  10 / 10  Loss:  tensor(1.0405, grad_fn=<DivBackward0>)\n",
      "Epoch:  256\n",
      "Training  10 / 10  Loss:  tensor(1.0532, grad_fn=<DivBackward0>)\n",
      "Epoch:  257\n",
      "Training  10 / 10  Loss:  tensor(1.0469, grad_fn=<DivBackward0>)\n",
      "Epoch:  258\n",
      "Training  10 / 10  Loss:  tensor(1.0821, grad_fn=<DivBackward0>)\n",
      "Epoch:  259\n",
      "Training  10 / 10  Loss:  tensor(1.0787, grad_fn=<DivBackward0>)\n",
      "Epoch:  260\n",
      "Training  10 / 10  Loss:  tensor(1.0428, grad_fn=<DivBackward0>)\n",
      "Epoch:  261\n",
      "Training  10 / 10  Loss:  tensor(1.0320, grad_fn=<DivBackward0>)\n",
      "Epoch:  262\n",
      "Training  10 / 10  Loss:  tensor(1.0414, grad_fn=<DivBackward0>)\n",
      "Epoch:  263\n",
      "Training  10 / 10  Loss:  tensor(1.0249, grad_fn=<DivBackward0>)\n",
      "Epoch:  264\n",
      "Training  10 / 10  Loss:  tensor(1.0693, grad_fn=<DivBackward0>)\n",
      "Epoch:  265\n",
      "Training  10 / 10  Loss:  tensor(1.0280, grad_fn=<DivBackward0>)\n",
      "Epoch:  266\n",
      "Training  10 / 10  Loss:  tensor(1.0118, grad_fn=<DivBackward0>)\n",
      "Epoch:  267\n",
      "Training  10 / 10  Loss:  tensor(1.0225, grad_fn=<DivBackward0>)\n",
      "Epoch:  268\n",
      "Training  10 / 10  Loss:  tensor(1.0704, grad_fn=<DivBackward0>)\n",
      "Epoch:  269\n",
      "Training  10 / 10  Loss:  tensor(1.0576, grad_fn=<DivBackward0>)\n",
      "Epoch:  270\n",
      "Training  10 / 10  Loss:  tensor(1.0774, grad_fn=<DivBackward0>)\n",
      "Epoch:  271\n",
      "Training  10 / 10  Loss:  tensor(1.0499, grad_fn=<DivBackward0>)\n",
      "Epoch:  272\n",
      "Training  10 / 10  Loss:  tensor(1.0659, grad_fn=<DivBackward0>)\n",
      "Epoch:  273\n",
      "Training  10 / 10  Loss:  tensor(1.0535, grad_fn=<DivBackward0>)\n",
      "Epoch:  274\n",
      "Training  10 / 10  Loss:  tensor(1.0788, grad_fn=<DivBackward0>)\n",
      "Epoch:  275\n",
      "Training  10 / 10  Loss:  tensor(1.0796, grad_fn=<DivBackward0>)\n",
      "Epoch:  276\n",
      "Training  10 / 10  Loss:  tensor(1.0900, grad_fn=<DivBackward0>)\n",
      "Epoch:  277\n",
      "Training  10 / 10  Loss:  tensor(1.0835, grad_fn=<DivBackward0>)\n",
      "Epoch:  278\n",
      "Training  10 / 10  Loss:  tensor(1.0600, grad_fn=<DivBackward0>)\n",
      "Epoch:  279\n",
      "Training  10 / 10  Loss:  tensor(1.0658, grad_fn=<DivBackward0>)\n",
      "Epoch:  280\n",
      "Training  10 / 10  Loss:  tensor(1.0685, grad_fn=<DivBackward0>)\n",
      "Epoch:  281\n",
      "Training  10 / 10  Loss:  tensor(1.0507, grad_fn=<DivBackward0>)\n",
      "Epoch:  282\n",
      "Training  10 / 10  Loss:  tensor(1.0503, grad_fn=<DivBackward0>)\n",
      "Epoch:  283\n",
      "Training  10 / 10  Loss:  tensor(1.0604, grad_fn=<DivBackward0>)\n",
      "Epoch:  284\n",
      "Training  10 / 10  Loss:  tensor(1.0152, grad_fn=<DivBackward0>)\n",
      "Epoch:  285\n",
      "Training  10 / 10  Loss:  tensor(1.0216, grad_fn=<DivBackward0>)\n",
      "Epoch:  286\n",
      "Training  10 / 10  Loss:  tensor(1.0148, grad_fn=<DivBackward0>)\n",
      "Epoch:  287\n",
      "Training  10 / 10  Loss:  tensor(1.0285, grad_fn=<DivBackward0>)\n",
      "Epoch:  288\n",
      "Training  10 / 10  Loss:  tensor(1.0103, grad_fn=<DivBackward0>)\n",
      "Epoch:  289\n",
      "Training  10 / 10  Loss:  tensor(1.0266, grad_fn=<DivBackward0>)\n",
      "Epoch:  290\n",
      "Training  10 / 10  Loss:  tensor(0.9983, grad_fn=<DivBackward0>)\n",
      "Epoch:  291\n",
      "Training  10 / 10  Loss:  tensor(1.0051, grad_fn=<DivBackward0>)\n",
      "Epoch:  292\n",
      "Training  10 / 10  Loss:  tensor(1.0003, grad_fn=<DivBackward0>)\n",
      "Epoch:  293\n",
      "Training  10 / 10  Loss:  tensor(1.0090, grad_fn=<DivBackward0>)\n",
      "Epoch:  294\n",
      "Training  10 / 10  Loss:  tensor(1.0253, grad_fn=<DivBackward0>)\n",
      "Epoch:  295\n",
      "Training  10 / 10  Loss:  tensor(1.0238, grad_fn=<DivBackward0>)\n",
      "Epoch:  296\n",
      "Training  10 / 10  Loss:  tensor(1.0231, grad_fn=<DivBackward0>)\n",
      "Epoch:  297\n",
      "Training  10 / 10  Loss:  tensor(1.0547, grad_fn=<DivBackward0>)\n",
      "Epoch:  298\n",
      "Training  10 / 10  Loss:  tensor(1.0646, grad_fn=<DivBackward0>)\n",
      "Epoch:  299\n",
      "Training  10 / 10  Loss:  tensor(1.0571, grad_fn=<DivBackward0>)\n",
      "Epoch:  300\n",
      "Training  10 / 10  Loss:  tensor(1.0396, grad_fn=<DivBackward0>)\n",
      "Epoch:  301\n",
      "Training  10 / 10  Loss:  tensor(1.0572, grad_fn=<DivBackward0>)\n",
      "Epoch:  302\n",
      "Training  10 / 10  Loss:  tensor(1.0744, grad_fn=<DivBackward0>)\n",
      "Epoch:  303\n",
      "Training  10 / 10  Loss:  tensor(1.0460, grad_fn=<DivBackward0>)\n",
      "Epoch:  304\n",
      "Training  10 / 10  Loss:  tensor(1.0476, grad_fn=<DivBackward0>)\n",
      "Epoch:  305\n",
      "Training  10 / 10  Loss:  tensor(1.0235, grad_fn=<DivBackward0>)\n",
      "Epoch:  306\n",
      "Training  10 / 10  Loss:  tensor(1.0214, grad_fn=<DivBackward0>)\n",
      "Epoch:  307\n",
      "Training  10 / 10  Loss:  tensor(1.0272, grad_fn=<DivBackward0>)\n",
      "Epoch:  308\n",
      "Training  10 / 10  Loss:  tensor(1.0243, grad_fn=<DivBackward0>)\n",
      "Epoch:  309\n",
      "Training  10 / 10  Loss:  tensor(1.0254, grad_fn=<DivBackward0>)\n",
      "Epoch:  310\n",
      "Training  10 / 10  Loss:  tensor(1.0209, grad_fn=<DivBackward0>)\n",
      "Epoch:  311\n",
      "Training  10 / 10  Loss:  tensor(1.0531, grad_fn=<DivBackward0>)\n",
      "Epoch:  312\n",
      "Training  10 / 10  Loss:  tensor(1.0556, grad_fn=<DivBackward0>)\n",
      "Epoch:  313\n",
      "Training  10 / 10  Loss:  tensor(1.0516, grad_fn=<DivBackward0>)\n",
      "Epoch:  314\n",
      "Training  10 / 10  Loss:  tensor(1.0208, grad_fn=<DivBackward0>)\n",
      "Epoch:  315\n",
      "Training  10 / 10  Loss:  tensor(1.0069, grad_fn=<DivBackward0>)\n",
      "Epoch:  316\n",
      "Training  10 / 10  Loss:  tensor(1.0292, grad_fn=<DivBackward0>)\n",
      "Epoch:  317\n",
      "Training  10 / 10  Loss:  tensor(0.9946, grad_fn=<DivBackward0>)\n",
      "Epoch:  318\n",
      "Training  10 / 10  Loss:  tensor(0.9867, grad_fn=<DivBackward0>)\n",
      "Epoch:  319\n",
      "Training  10 / 10  Loss:  tensor(1.0426, grad_fn=<DivBackward0>)\n",
      "Epoch:  320\n",
      "Training  10 / 10  Loss:  tensor(1.0039, grad_fn=<DivBackward0>)\n",
      "Epoch:  321\n",
      "Training  10 / 10  Loss:  tensor(1.0003, grad_fn=<DivBackward0>)\n",
      "Epoch:  322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(1.0123, grad_fn=<DivBackward0>)\n",
      "Epoch:  323\n",
      "Training  10 / 10  Loss:  tensor(1.0101, grad_fn=<DivBackward0>)\n",
      "Epoch:  324\n",
      "Training  10 / 10  Loss:  tensor(1.0334, grad_fn=<DivBackward0>)\n",
      "Epoch:  325\n",
      "Training  10 / 10  Loss:  tensor(1.0215, grad_fn=<DivBackward0>)\n",
      "Epoch:  326\n",
      "Training  10 / 10  Loss:  tensor(1.0559, grad_fn=<DivBackward0>)\n",
      "Epoch:  327\n",
      "Training  10 / 10  Loss:  tensor(1.0406, grad_fn=<DivBackward0>)\n",
      "Epoch:  328\n",
      "Training  10 / 10  Loss:  tensor(1.0136, grad_fn=<DivBackward0>)\n",
      "Epoch:  329\n",
      "Training  10 / 10  Loss:  tensor(1.0275, grad_fn=<DivBackward0>)\n",
      "Epoch:  330\n",
      "Training  10 / 10  Loss:  tensor(0.9907, grad_fn=<DivBackward0>)\n",
      "Epoch:  331\n",
      "Training  10 / 10  Loss:  tensor(0.9944, grad_fn=<DivBackward0>)\n",
      "Epoch:  332\n",
      "Training  10 / 10  Loss:  tensor(1.0239, grad_fn=<DivBackward0>)\n",
      "Epoch:  333\n",
      "Training  10 / 10  Loss:  tensor(1.0197, grad_fn=<DivBackward0>)\n",
      "Epoch:  334\n",
      "Training  10 / 10  Loss:  tensor(1.0410, grad_fn=<DivBackward0>)\n",
      "Epoch:  335\n",
      "Training  10 / 10  Loss:  tensor(1.0420, grad_fn=<DivBackward0>)\n",
      "Epoch:  336\n",
      "Training  10 / 10  Loss:  tensor(1.0124, grad_fn=<DivBackward0>)\n",
      "Epoch:  337\n",
      "Training  10 / 10  Loss:  tensor(1.0401, grad_fn=<DivBackward0>)\n",
      "Epoch:  338\n",
      "Training  10 / 10  Loss:  tensor(1.0282, grad_fn=<DivBackward0>)\n",
      "Epoch:  339\n",
      "Training  10 / 10  Loss:  tensor(1.0263, grad_fn=<DivBackward0>)\n",
      "Epoch:  340\n",
      "Training  10 / 10  Loss:  tensor(1.0333, grad_fn=<DivBackward0>)\n",
      "Epoch:  341\n",
      "Training  10 / 10  Loss:  tensor(0.9934, grad_fn=<DivBackward0>)\n",
      "Epoch:  342\n",
      "Training  10 / 10  Loss:  tensor(1.0250, grad_fn=<DivBackward0>)\n",
      "Epoch:  343\n",
      "Training  10 / 10  Loss:  tensor(1.0360, grad_fn=<DivBackward0>)\n",
      "Epoch:  344\n",
      "Training  10 / 10  Loss:  tensor(1.0283, grad_fn=<DivBackward0>)\n",
      "Epoch:  345\n",
      "Training  10 / 10  Loss:  tensor(1.0067, grad_fn=<DivBackward0>)\n",
      "Epoch:  346\n",
      "Training  10 / 10  Loss:  tensor(1.0326, grad_fn=<DivBackward0>)\n",
      "Epoch:  347\n",
      "Training  10 / 10  Loss:  tensor(1.0171, grad_fn=<DivBackward0>)\n",
      "Epoch:  348\n",
      "Training  10 / 10  Loss:  tensor(1.0512, grad_fn=<DivBackward0>)\n",
      "Epoch:  349\n",
      "Training  10 / 10  Loss:  tensor(0.9927, grad_fn=<DivBackward0>)\n",
      "Epoch:  350\n",
      "Training  10 / 10  Loss:  tensor(1.0267, grad_fn=<DivBackward0>)\n",
      "Epoch:  351\n",
      "Training  10 / 10  Loss:  tensor(1.0304, grad_fn=<DivBackward0>)\n",
      "Epoch:  352\n",
      "Training  10 / 10  Loss:  tensor(1.0358, grad_fn=<DivBackward0>)\n",
      "Epoch:  353\n",
      "Training  10 / 10  Loss:  tensor(1.0169, grad_fn=<DivBackward0>)\n",
      "Epoch:  354\n",
      "Training  10 / 10  Loss:  tensor(0.9869, grad_fn=<DivBackward0>)\n",
      "Epoch:  355\n",
      "Training  10 / 10  Loss:  tensor(1.0023, grad_fn=<DivBackward0>)\n",
      "Epoch:  356\n",
      "Training  10 / 10  Loss:  tensor(1.0101, grad_fn=<DivBackward0>)\n",
      "Epoch:  357\n",
      "Training  10 / 10  Loss:  tensor(1.0023, grad_fn=<DivBackward0>)\n",
      "Epoch:  358\n",
      "Training  10 / 10  Loss:  tensor(0.9639, grad_fn=<DivBackward0>)\n",
      "Epoch:  359\n",
      "Training  10 / 10  Loss:  tensor(0.9630, grad_fn=<DivBackward0>)\n",
      "Epoch:  360\n",
      "Training  10 / 10  Loss:  tensor(1.0005, grad_fn=<DivBackward0>)\n",
      "Epoch:  361\n",
      "Training  10 / 10  Loss:  tensor(0.9937, grad_fn=<DivBackward0>)\n",
      "Epoch:  362\n",
      "Training  10 / 10  Loss:  tensor(0.9733, grad_fn=<DivBackward0>)\n",
      "Epoch:  363\n",
      "Training  10 / 10  Loss:  tensor(0.9748, grad_fn=<DivBackward0>)\n",
      "Epoch:  364\n",
      "Training  10 / 10  Loss:  tensor(0.9736, grad_fn=<DivBackward0>)\n",
      "Epoch:  365\n",
      "Training  10 / 10  Loss:  tensor(0.9756, grad_fn=<DivBackward0>)\n",
      "Epoch:  366\n",
      "Training  10 / 10  Loss:  tensor(0.9721, grad_fn=<DivBackward0>)\n",
      "Epoch:  367\n",
      "Training  10 / 10  Loss:  tensor(0.9848, grad_fn=<DivBackward0>)\n",
      "Epoch:  368\n",
      "Training  10 / 10  Loss:  tensor(0.9973, grad_fn=<DivBackward0>)\n",
      "Epoch:  369\n",
      "Training  10 / 10  Loss:  tensor(0.9519, grad_fn=<DivBackward0>)\n",
      "Epoch:  370\n",
      "Training  10 / 10  Loss:  tensor(0.9465, grad_fn=<DivBackward0>)\n",
      "Epoch:  371\n",
      "Training  10 / 10  Loss:  tensor(0.9498, grad_fn=<DivBackward0>)\n",
      "Epoch:  372\n",
      "Training  10 / 10  Loss:  tensor(0.9737, grad_fn=<DivBackward0>)\n",
      "Epoch:  373\n",
      "Training  10 / 10  Loss:  tensor(0.9771, grad_fn=<DivBackward0>)\n",
      "Epoch:  374\n",
      "Training  10 / 10  Loss:  tensor(0.9668, grad_fn=<DivBackward0>)\n",
      "Epoch:  375\n",
      "Training  10 / 10  Loss:  tensor(0.9564, grad_fn=<DivBackward0>)\n",
      "Epoch:  376\n",
      "Training  10 / 10  Loss:  tensor(0.9515, grad_fn=<DivBackward0>)\n",
      "Epoch:  377\n",
      "Training  10 / 10  Loss:  tensor(0.9645, grad_fn=<DivBackward0>)\n",
      "Epoch:  378\n",
      "Training  10 / 10  Loss:  tensor(0.9618, grad_fn=<DivBackward0>)\n",
      "Epoch:  379\n",
      "Training  10 / 10  Loss:  tensor(0.9831, grad_fn=<DivBackward0>)\n",
      "Epoch:  380\n",
      "Training  10 / 10  Loss:  tensor(0.9696, grad_fn=<DivBackward0>)\n",
      "Epoch:  381\n",
      "Training  10 / 10  Loss:  tensor(0.9594, grad_fn=<DivBackward0>)\n",
      "Epoch:  382\n",
      "Training  10 / 10  Loss:  tensor(0.9547, grad_fn=<DivBackward0>)\n",
      "Epoch:  383\n",
      "Training  10 / 10  Loss:  tensor(0.9551, grad_fn=<DivBackward0>)\n",
      "Epoch:  384\n",
      "Training  10 / 10  Loss:  tensor(0.9452, grad_fn=<DivBackward0>)\n",
      "Epoch:  385\n",
      "Training  10 / 10  Loss:  tensor(0.9756, grad_fn=<DivBackward0>)\n",
      "Epoch:  386\n",
      "Training  10 / 10  Loss:  tensor(0.9725, grad_fn=<DivBackward0>)\n",
      "Epoch:  387\n",
      "Training  10 / 10  Loss:  tensor(1.0138, grad_fn=<DivBackward0>)\n",
      "Epoch:  388\n",
      "Training  10 / 10  Loss:  tensor(1.0114, grad_fn=<DivBackward0>)\n",
      "Epoch:  389\n",
      "Training  10 / 10  Loss:  tensor(1.0223, grad_fn=<DivBackward0>)\n",
      "Epoch:  390\n",
      "Training  10 / 10  Loss:  tensor(0.9877, grad_fn=<DivBackward0>)\n",
      "Epoch:  391\n",
      "Training  10 / 10  Loss:  tensor(1.0056, grad_fn=<DivBackward0>)\n",
      "Epoch:  392\n",
      "Training  10 / 10  Loss:  tensor(1.0131, grad_fn=<DivBackward0>)\n",
      "Epoch:  393\n",
      "Training  10 / 10  Loss:  tensor(1.0222, grad_fn=<DivBackward0>)\n",
      "Epoch:  394\n",
      "Training  10 / 10  Loss:  tensor(1.0221, grad_fn=<DivBackward0>)\n",
      "Epoch:  395\n",
      "Training  10 / 10  Loss:  tensor(1.0244, grad_fn=<DivBackward0>)\n",
      "Epoch:  396\n",
      "Training  10 / 10  Loss:  tensor(0.9860, grad_fn=<DivBackward0>)\n",
      "Epoch:  397\n",
      "Training  10 / 10  Loss:  tensor(0.9905, grad_fn=<DivBackward0>)\n",
      "Epoch:  398\n",
      "Training  10 / 10  Loss:  tensor(0.9843, grad_fn=<DivBackward0>)\n",
      "Epoch:  399\n",
      "Training  10 / 10  Loss:  tensor(0.9655, grad_fn=<DivBackward0>)\n",
      "Epoch:  400\n",
      "Training  10 / 10  Loss:  tensor(0.9803, grad_fn=<DivBackward0>)\n",
      "Epoch:  401\n",
      "Training  10 / 10  Loss:  tensor(0.9752, grad_fn=<DivBackward0>)\n",
      "Epoch:  402\n",
      "Training  10 / 10  Loss:  tensor(0.9511, grad_fn=<DivBackward0>)\n",
      "Epoch:  403\n",
      "Training  10 / 10  Loss:  tensor(0.9389, grad_fn=<DivBackward0>)\n",
      "Epoch:  404\n",
      "Training  10 / 10  Loss:  tensor(0.9770, grad_fn=<DivBackward0>)\n",
      "Epoch:  405\n",
      "Training  10 / 10  Loss:  tensor(0.9564, grad_fn=<DivBackward0>)\n",
      "Epoch:  406\n",
      "Training  10 / 10  Loss:  tensor(0.9497, grad_fn=<DivBackward0>)\n",
      "Epoch:  407\n",
      "Training  10 / 10  Loss:  tensor(0.9697, grad_fn=<DivBackward0>)\n",
      "Epoch:  408\n",
      "Training  10 / 10  Loss:  tensor(0.9559, grad_fn=<DivBackward0>)\n",
      "Epoch:  409\n",
      "Training  10 / 10  Loss:  tensor(0.9769, grad_fn=<DivBackward0>)\n",
      "Epoch:  410\n",
      "Training  10 / 10  Loss:  tensor(0.9980, grad_fn=<DivBackward0>)\n",
      "Epoch:  411\n",
      "Training  10 / 10  Loss:  tensor(1.0521, grad_fn=<DivBackward0>)\n",
      "Epoch:  412\n",
      "Training  10 / 10  Loss:  tensor(1.0104, grad_fn=<DivBackward0>)\n",
      "Epoch:  413\n",
      "Training  10 / 10  Loss:  tensor(0.9909, grad_fn=<DivBackward0>)\n",
      "Epoch:  414\n",
      "Training  10 / 10  Loss:  tensor(0.9557, grad_fn=<DivBackward0>)\n",
      "Epoch:  415\n",
      "Training  10 / 10  Loss:  tensor(0.9491, grad_fn=<DivBackward0>)\n",
      "Epoch:  416\n",
      "Training  10 / 10  Loss:  tensor(0.9361, grad_fn=<DivBackward0>)\n",
      "Epoch:  417\n",
      "Training  10 / 10  Loss:  tensor(0.9269, grad_fn=<DivBackward0>)\n",
      "Epoch:  418\n",
      "Training  10 / 10  Loss:  tensor(0.9330, grad_fn=<DivBackward0>)\n",
      "Epoch:  419\n",
      "Training  10 / 10  Loss:  tensor(0.9306, grad_fn=<DivBackward0>)\n",
      "Epoch:  420\n",
      "Training  10 / 10  Loss:  tensor(0.9351, grad_fn=<DivBackward0>)\n",
      "Epoch:  421\n",
      "Training  10 / 10  Loss:  tensor(0.9257, grad_fn=<DivBackward0>)\n",
      "Epoch:  422\n",
      "Training  10 / 10  Loss:  tensor(0.9336, grad_fn=<DivBackward0>)\n",
      "Epoch:  423\n",
      "Training  10 / 10  Loss:  tensor(0.9213, grad_fn=<DivBackward0>)\n",
      "Epoch:  424\n",
      "Training  10 / 10  Loss:  tensor(0.9179, grad_fn=<DivBackward0>)\n",
      "Epoch:  425\n",
      "Training  10 / 10  Loss:  tensor(0.9107, grad_fn=<DivBackward0>)\n",
      "Epoch:  426\n",
      "Training  10 / 10  Loss:  tensor(0.9172, grad_fn=<DivBackward0>)\n",
      "Epoch:  427\n",
      "Training  10 / 10  Loss:  tensor(0.9204, grad_fn=<DivBackward0>)\n",
      "Epoch:  428\n",
      "Training  10 / 10  Loss:  tensor(0.9103, grad_fn=<DivBackward0>)\n",
      "Epoch:  429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(0.9107, grad_fn=<DivBackward0>)\n",
      "Epoch:  430\n",
      "Training  10 / 10  Loss:  tensor(0.9132, grad_fn=<DivBackward0>)\n",
      "Epoch:  431\n",
      "Training  10 / 10  Loss:  tensor(0.9233, grad_fn=<DivBackward0>)\n",
      "Epoch:  432\n",
      "Training  10 / 10  Loss:  tensor(0.9109, grad_fn=<DivBackward0>)\n",
      "Epoch:  433\n",
      "Training  10 / 10  Loss:  tensor(0.9193, grad_fn=<DivBackward0>)\n",
      "Epoch:  434\n",
      "Training  10 / 10  Loss:  tensor(0.9136, grad_fn=<DivBackward0>)\n",
      "Epoch:  435\n",
      "Training  10 / 10  Loss:  tensor(0.9324, grad_fn=<DivBackward0>)\n",
      "Epoch:  436\n",
      "Training  10 / 10  Loss:  tensor(0.9243, grad_fn=<DivBackward0>)\n",
      "Epoch:  437\n",
      "Training  10 / 10  Loss:  tensor(0.9260, grad_fn=<DivBackward0>)\n",
      "Epoch:  438\n",
      "Training  10 / 10  Loss:  tensor(0.9136, grad_fn=<DivBackward0>)\n",
      "Epoch:  439\n",
      "Training  10 / 10  Loss:  tensor(0.9128, grad_fn=<DivBackward0>)\n",
      "Epoch:  440\n",
      "Training  10 / 10  Loss:  tensor(0.9136, grad_fn=<DivBackward0>)\n",
      "Epoch:  441\n",
      "Training  10 / 10  Loss:  tensor(0.9220, grad_fn=<DivBackward0>)\n",
      "Epoch:  442\n",
      "Training  10 / 10  Loss:  tensor(0.9316, grad_fn=<DivBackward0>)\n",
      "Epoch:  443\n",
      "Training  10 / 10  Loss:  tensor(0.9136, grad_fn=<DivBackward0>)\n",
      "Epoch:  444\n",
      "Training  10 / 10  Loss:  tensor(0.9117, grad_fn=<DivBackward0>)\n",
      "Epoch:  445\n",
      "Training  10 / 10  Loss:  tensor(0.9181, grad_fn=<DivBackward0>)\n",
      "Epoch:  446\n",
      "Training  10 / 10  Loss:  tensor(0.9214, grad_fn=<DivBackward0>)\n",
      "Epoch:  447\n",
      "Training  10 / 10  Loss:  tensor(0.9091, grad_fn=<DivBackward0>)\n",
      "Epoch:  448\n",
      "Training  10 / 10  Loss:  tensor(0.9119, grad_fn=<DivBackward0>)\n",
      "Epoch:  449\n",
      "Training  10 / 10  Loss:  tensor(0.9188, grad_fn=<DivBackward0>)\n",
      "Epoch:  450\n",
      "Training  10 / 10  Loss:  tensor(0.9114, grad_fn=<DivBackward0>)\n",
      "Epoch:  451\n",
      "Training  10 / 10  Loss:  tensor(0.9124, grad_fn=<DivBackward0>)\n",
      "Epoch:  452\n",
      "Training  10 / 10  Loss:  tensor(0.9090, grad_fn=<DivBackward0>)\n",
      "Epoch:  453\n",
      "Training  10 / 10  Loss:  tensor(0.9090, grad_fn=<DivBackward0>)\n",
      "Epoch:  454\n",
      "Training  10 / 10  Loss:  tensor(0.9173, grad_fn=<DivBackward0>)\n",
      "Epoch:  455\n",
      "Training  10 / 10  Loss:  tensor(0.9116, grad_fn=<DivBackward0>)\n",
      "Epoch:  456\n",
      "Training  10 / 10  Loss:  tensor(0.9070, grad_fn=<DivBackward0>)\n",
      "Epoch:  457\n",
      "Training  10 / 10  Loss:  tensor(0.9077, grad_fn=<DivBackward0>)\n",
      "Epoch:  458\n",
      "Training  10 / 10  Loss:  tensor(0.9157, grad_fn=<DivBackward0>)\n",
      "Epoch:  459\n",
      "Training  10 / 10  Loss:  tensor(0.9146, grad_fn=<DivBackward0>)\n",
      "Epoch:  460\n",
      "Training  10 / 10  Loss:  tensor(0.9151, grad_fn=<DivBackward0>)\n",
      "Epoch:  461\n",
      "Training  10 / 10  Loss:  tensor(0.9128, grad_fn=<DivBackward0>)\n",
      "Epoch:  462\n",
      "Training  10 / 10  Loss:  tensor(0.9238, grad_fn=<DivBackward0>)\n",
      "Epoch:  463\n",
      "Training  10 / 10  Loss:  tensor(0.9171, grad_fn=<DivBackward0>)\n",
      "Epoch:  464\n",
      "Training  10 / 10  Loss:  tensor(0.9286, grad_fn=<DivBackward0>)\n",
      "Epoch:  465\n",
      "Training  10 / 10  Loss:  tensor(0.9182, grad_fn=<DivBackward0>)\n",
      "Epoch:  466\n",
      "Training  10 / 10  Loss:  tensor(0.9220, grad_fn=<DivBackward0>)\n",
      "Epoch:  467\n",
      "Training  10 / 10  Loss:  tensor(0.9100, grad_fn=<DivBackward0>)\n",
      "Epoch:  468\n",
      "Training  10 / 10  Loss:  tensor(0.9122, grad_fn=<DivBackward0>)\n",
      "Epoch:  469\n",
      "Training  10 / 10  Loss:  tensor(0.9182, grad_fn=<DivBackward0>)\n",
      "Epoch:  470\n",
      "Training  10 / 10  Loss:  tensor(0.9182, grad_fn=<DivBackward0>)\n",
      "Epoch:  471\n",
      "Training  10 / 10  Loss:  tensor(0.9190, grad_fn=<DivBackward0>)\n",
      "Epoch:  472\n",
      "Training  10 / 10  Loss:  tensor(0.9162, grad_fn=<DivBackward0>)\n",
      "Epoch:  473\n",
      "Training  10 / 10  Loss:  tensor(0.9154, grad_fn=<DivBackward0>)\n",
      "Epoch:  474\n",
      "Training  10 / 10  Loss:  tensor(0.9178, grad_fn=<DivBackward0>)\n",
      "Epoch:  475\n",
      "Training  10 / 10  Loss:  tensor(0.9156, grad_fn=<DivBackward0>)\n",
      "Epoch:  476\n",
      "Training  10 / 10  Loss:  tensor(0.9024, grad_fn=<DivBackward0>)\n",
      "Epoch:  477\n",
      "Training  10 / 10  Loss:  tensor(0.9133, grad_fn=<DivBackward0>)\n",
      "Epoch:  478\n",
      "Training  10 / 10  Loss:  tensor(0.9127, grad_fn=<DivBackward0>)\n",
      "Epoch:  479\n",
      "Training  10 / 10  Loss:  tensor(0.9123, grad_fn=<DivBackward0>)\n",
      "Epoch:  480\n",
      "Training  10 / 10  Loss:  tensor(0.9052, grad_fn=<DivBackward0>)\n",
      "Epoch:  481\n",
      "Training  10 / 10  Loss:  tensor(0.9105, grad_fn=<DivBackward0>)\n",
      "Epoch:  482\n",
      "Training  10 / 10  Loss:  tensor(0.9132, grad_fn=<DivBackward0>)\n",
      "Epoch:  483\n",
      "Training  10 / 10  Loss:  tensor(0.9083, grad_fn=<DivBackward0>)\n",
      "Epoch:  484\n",
      "Training  10 / 10  Loss:  tensor(0.9045, grad_fn=<DivBackward0>)\n",
      "Epoch:  485\n",
      "Training  10 / 10  Loss:  tensor(0.9045, grad_fn=<DivBackward0>)\n",
      "Epoch:  486\n",
      "Training  10 / 10  Loss:  tensor(0.9031, grad_fn=<DivBackward0>)\n",
      "Epoch:  487\n",
      "Training  10 / 10  Loss:  tensor(0.9061, grad_fn=<DivBackward0>)\n",
      "Epoch:  488\n",
      "Training  10 / 10  Loss:  tensor(0.9098, grad_fn=<DivBackward0>)\n",
      "Epoch:  489\n",
      "Training  10 / 10  Loss:  tensor(0.9116, grad_fn=<DivBackward0>)\n",
      "Epoch:  490\n",
      "Training  10 / 10  Loss:  tensor(0.9087, grad_fn=<DivBackward0>)\n",
      "Epoch:  491\n",
      "Training  10 / 10  Loss:  tensor(0.9057, grad_fn=<DivBackward0>)\n",
      "Epoch:  492\n",
      "Training  10 / 10  Loss:  tensor(0.9104, grad_fn=<DivBackward0>)\n",
      "Epoch:  493\n",
      "Training  10 / 10  Loss:  tensor(0.9018, grad_fn=<DivBackward0>)\n",
      "Epoch:  494\n",
      "Training  10 / 10  Loss:  tensor(0.9045, grad_fn=<DivBackward0>)\n",
      "Epoch:  495\n",
      "Training  10 / 10  Loss:  tensor(0.9006, grad_fn=<DivBackward0>)\n",
      "Epoch:  496\n",
      "Training  10 / 10  Loss:  tensor(0.9070, grad_fn=<DivBackward0>)\n",
      "Epoch:  497\n",
      "Training  10 / 10  Loss:  tensor(0.9120, grad_fn=<DivBackward0>)\n",
      "Epoch:  498\n",
      "Training  10 / 10  Loss:  tensor(0.9054, grad_fn=<DivBackward0>)\n",
      "Epoch:  499\n",
      "Training  10 / 10  Loss:  tensor(0.9040, grad_fn=<DivBackward0>)\n",
      "Epoch:  500\n",
      "Training  10 / 10  Loss:  tensor(0.9000, grad_fn=<DivBackward0>)\n",
      "Epoch:  501\n",
      "Training  10 / 10  Loss:  tensor(0.9057, grad_fn=<DivBackward0>)\n",
      "Epoch:  502\n",
      "Training  10 / 10  Loss:  tensor(0.9046, grad_fn=<DivBackward0>)\n",
      "Epoch:  503\n",
      "Training  10 / 10  Loss:  tensor(0.9062, grad_fn=<DivBackward0>)\n",
      "Epoch:  504\n",
      "Training  10 / 10  Loss:  tensor(0.9044, grad_fn=<DivBackward0>)\n",
      "Epoch:  505\n",
      "Training  10 / 10  Loss:  tensor(0.9035, grad_fn=<DivBackward0>)\n",
      "Epoch:  506\n",
      "Training  10 / 10  Loss:  tensor(0.9020, grad_fn=<DivBackward0>)\n",
      "Epoch:  507\n",
      "Training  10 / 10  Loss:  tensor(0.9029, grad_fn=<DivBackward0>)\n",
      "Epoch:  508\n",
      "Training  10 / 10  Loss:  tensor(0.9081, grad_fn=<DivBackward0>)\n",
      "Epoch:  509\n",
      "Training  10 / 10  Loss:  tensor(0.9008, grad_fn=<DivBackward0>)\n",
      "Epoch:  510\n",
      "Training  10 / 10  Loss:  tensor(0.8990, grad_fn=<DivBackward0>)\n",
      "Epoch:  511\n",
      "Training  10 / 10  Loss:  tensor(0.9007, grad_fn=<DivBackward0>)\n",
      "Epoch:  512\n",
      "Training  10 / 10  Loss:  tensor(0.9057, grad_fn=<DivBackward0>)\n",
      "Epoch:  513\n",
      "Training  10 / 10  Loss:  tensor(0.9045, grad_fn=<DivBackward0>)\n",
      "Epoch:  514\n",
      "Training  10 / 10  Loss:  tensor(0.9010, grad_fn=<DivBackward0>)\n",
      "Epoch:  515\n",
      "Training  10 / 10  Loss:  tensor(0.9002, grad_fn=<DivBackward0>)\n",
      "Epoch:  516\n",
      "Training  10 / 10  Loss:  tensor(0.9019, grad_fn=<DivBackward0>)\n",
      "Epoch:  517\n",
      "Training  10 / 10  Loss:  tensor(0.9070, grad_fn=<DivBackward0>)\n",
      "Epoch:  518\n",
      "Training  10 / 10  Loss:  tensor(0.9000, grad_fn=<DivBackward0>)\n",
      "Epoch:  519\n",
      "Training  10 / 10  Loss:  tensor(0.8988, grad_fn=<DivBackward0>)\n",
      "Epoch:  520\n",
      "Training  10 / 10  Loss:  tensor(0.9003, grad_fn=<DivBackward0>)\n",
      "Epoch:  521\n",
      "Training  10 / 10  Loss:  tensor(0.9064, grad_fn=<DivBackward0>)\n",
      "Epoch:  522\n",
      "Training  10 / 10  Loss:  tensor(0.9053, grad_fn=<DivBackward0>)\n",
      "Epoch:  523\n",
      "Training  10 / 10  Loss:  tensor(0.9031, grad_fn=<DivBackward0>)\n",
      "Epoch:  524\n",
      "Training  10 / 10  Loss:  tensor(0.9077, grad_fn=<DivBackward0>)\n",
      "Epoch:  525\n",
      "Training  10 / 10  Loss:  tensor(0.9125, grad_fn=<DivBackward0>)\n",
      "Epoch:  526\n",
      "Training  10 / 10  Loss:  tensor(0.9038, grad_fn=<DivBackward0>)\n",
      "Epoch:  527\n",
      "Training  10 / 10  Loss:  tensor(0.9013, grad_fn=<DivBackward0>)\n",
      "Epoch:  528\n",
      "Training  10 / 10  Loss:  tensor(0.8961, grad_fn=<DivBackward0>)\n",
      "Epoch:  529\n",
      "Training  10 / 10  Loss:  tensor(0.8977, grad_fn=<DivBackward0>)\n",
      "Epoch:  530\n",
      "Training  10 / 10  Loss:  tensor(0.9029, grad_fn=<DivBackward0>)\n",
      "Epoch:  531\n",
      "Training  10 / 10  Loss:  tensor(0.9007, grad_fn=<DivBackward0>)\n",
      "Epoch:  532\n",
      "Training  10 / 10  Loss:  tensor(0.9022, grad_fn=<DivBackward0>)\n",
      "Epoch:  533\n",
      "Training  10 / 10  Loss:  tensor(0.9028, grad_fn=<DivBackward0>)\n",
      "Epoch:  534\n",
      "Training  10 / 10  Loss:  tensor(0.9021, grad_fn=<DivBackward0>)\n",
      "Epoch:  535\n",
      "Training  10 / 10  Loss:  tensor(0.9009, grad_fn=<DivBackward0>)\n",
      "Epoch:  536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(0.9043, grad_fn=<DivBackward0>)\n",
      "Epoch:  537\n",
      "Training  10 / 10  Loss:  tensor(0.9022, grad_fn=<DivBackward0>)\n",
      "Epoch:  538\n",
      "Training  10 / 10  Loss:  tensor(0.9043, grad_fn=<DivBackward0>)\n",
      "Epoch:  539\n",
      "Training  10 / 10  Loss:  tensor(0.9032, grad_fn=<DivBackward0>)\n",
      "Epoch:  540\n",
      "Training  10 / 10  Loss:  tensor(0.9051, grad_fn=<DivBackward0>)\n",
      "Epoch:  541\n",
      "Training  10 / 10  Loss:  tensor(0.8986, grad_fn=<DivBackward0>)\n",
      "Epoch:  542\n",
      "Training  10 / 10  Loss:  tensor(0.8980, grad_fn=<DivBackward0>)\n",
      "Epoch:  543\n",
      "Training  10 / 10  Loss:  tensor(0.8951, grad_fn=<DivBackward0>)\n",
      "Epoch:  544\n",
      "Training  10 / 10  Loss:  tensor(0.9007, grad_fn=<DivBackward0>)\n",
      "Epoch:  545\n",
      "Training  10 / 10  Loss:  tensor(0.8997, grad_fn=<DivBackward0>)\n",
      "Epoch:  546\n",
      "Training  10 / 10  Loss:  tensor(0.8955, grad_fn=<DivBackward0>)\n",
      "Epoch:  547\n",
      "Training  10 / 10  Loss:  tensor(0.8962, grad_fn=<DivBackward0>)\n",
      "Epoch:  548\n",
      "Training  10 / 10  Loss:  tensor(0.8983, grad_fn=<DivBackward0>)\n",
      "Epoch:  549\n",
      "Training  10 / 10  Loss:  tensor(0.8982, grad_fn=<DivBackward0>)\n",
      "Epoch:  550\n",
      "Training  10 / 10  Loss:  tensor(0.8966, grad_fn=<DivBackward0>)\n",
      "Epoch:  551\n",
      "Training  10 / 10  Loss:  tensor(0.8980, grad_fn=<DivBackward0>)\n",
      "Epoch:  552\n",
      "Training  10 / 10  Loss:  tensor(0.8964, grad_fn=<DivBackward0>)\n",
      "Epoch:  553\n",
      "Training  10 / 10  Loss:  tensor(0.9006, grad_fn=<DivBackward0>)\n",
      "Epoch:  554\n",
      "Training  10 / 10  Loss:  tensor(0.8986, grad_fn=<DivBackward0>)\n",
      "Epoch:  555\n",
      "Training  10 / 10  Loss:  tensor(0.8967, grad_fn=<DivBackward0>)\n",
      "Epoch:  556\n",
      "Training  10 / 10  Loss:  tensor(0.8971, grad_fn=<DivBackward0>)\n",
      "Epoch:  557\n",
      "Training  10 / 10  Loss:  tensor(0.8936, grad_fn=<DivBackward0>)\n",
      "Epoch:  558\n",
      "Training  10 / 10  Loss:  tensor(0.8959, grad_fn=<DivBackward0>)\n",
      "Epoch:  559\n",
      "Training  10 / 10  Loss:  tensor(0.8974, grad_fn=<DivBackward0>)\n",
      "Epoch:  560\n",
      "Training  10 / 10  Loss:  tensor(0.8972, grad_fn=<DivBackward0>)\n",
      "Epoch:  561\n",
      "Training  10 / 10  Loss:  tensor(0.9011, grad_fn=<DivBackward0>)\n",
      "Epoch:  562\n",
      "Training  10 / 10  Loss:  tensor(0.8957, grad_fn=<DivBackward0>)\n",
      "Epoch:  563\n",
      "Training  10 / 10  Loss:  tensor(0.8970, grad_fn=<DivBackward0>)\n",
      "Epoch:  564\n",
      "Training  10 / 10  Loss:  tensor(0.8975, grad_fn=<DivBackward0>)\n",
      "Epoch:  565\n",
      "Training  10 / 10  Loss:  tensor(0.8944, grad_fn=<DivBackward0>)\n",
      "Epoch:  566\n",
      "Training  10 / 10  Loss:  tensor(0.8956, grad_fn=<DivBackward0>)\n",
      "Epoch:  567\n",
      "Training  10 / 10  Loss:  tensor(0.8958, grad_fn=<DivBackward0>)\n",
      "Epoch:  568\n",
      "Training  10 / 10  Loss:  tensor(0.8954, grad_fn=<DivBackward0>)\n",
      "Epoch:  569\n",
      "Training  10 / 10  Loss:  tensor(0.8983, grad_fn=<DivBackward0>)\n",
      "Epoch:  570\n",
      "Training  10 / 10  Loss:  tensor(0.8969, grad_fn=<DivBackward0>)\n",
      "Epoch:  571\n",
      "Training  10 / 10  Loss:  tensor(0.8953, grad_fn=<DivBackward0>)\n",
      "Epoch:  572\n",
      "Training  10 / 10  Loss:  tensor(0.8962, grad_fn=<DivBackward0>)\n",
      "Epoch:  573\n",
      "Training  10 / 10  Loss:  tensor(0.8971, grad_fn=<DivBackward0>)\n",
      "Epoch:  574\n",
      "Training  10 / 10  Loss:  tensor(0.8956, grad_fn=<DivBackward0>)\n",
      "Epoch:  575\n",
      "Training  10 / 10  Loss:  tensor(0.8974, grad_fn=<DivBackward0>)\n",
      "Epoch:  576\n",
      "Training  10 / 10  Loss:  tensor(0.9033, grad_fn=<DivBackward0>)\n",
      "Epoch:  577\n",
      "Training  10 / 10  Loss:  tensor(0.8978, grad_fn=<DivBackward0>)\n",
      "Epoch:  578\n",
      "Training  10 / 10  Loss:  tensor(0.8948, grad_fn=<DivBackward0>)\n",
      "Epoch:  579\n",
      "Training  10 / 10  Loss:  tensor(0.8977, grad_fn=<DivBackward0>)\n",
      "Epoch:  580\n",
      "Training  10 / 10  Loss:  tensor(0.8968, grad_fn=<DivBackward0>)\n",
      "Epoch:  581\n",
      "Training  10 / 10  Loss:  tensor(0.8996, grad_fn=<DivBackward0>)\n",
      "Epoch:  582\n",
      "Training  10 / 10  Loss:  tensor(0.8994, grad_fn=<DivBackward0>)\n",
      "Epoch:  583\n",
      "Training  10 / 10  Loss:  tensor(0.8949, grad_fn=<DivBackward0>)\n",
      "Epoch:  584\n",
      "Training  10 / 10  Loss:  tensor(0.8978, grad_fn=<DivBackward0>)\n",
      "Epoch:  585\n",
      "Training  10 / 10  Loss:  tensor(0.9017, grad_fn=<DivBackward0>)\n",
      "Epoch:  586\n",
      "Training  10 / 10  Loss:  tensor(0.8954, grad_fn=<DivBackward0>)\n",
      "Epoch:  587\n",
      "Training  10 / 10  Loss:  tensor(0.8969, grad_fn=<DivBackward0>)\n",
      "Epoch:  588\n",
      "Training  10 / 10  Loss:  tensor(0.8971, grad_fn=<DivBackward0>)\n",
      "Epoch:  589\n",
      "Training  10 / 10  Loss:  tensor(0.8930, grad_fn=<DivBackward0>)\n",
      "Epoch:  590\n",
      "Training  10 / 10  Loss:  tensor(0.8963, grad_fn=<DivBackward0>)\n",
      "Epoch:  591\n",
      "Training  10 / 10  Loss:  tensor(0.9000, grad_fn=<DivBackward0>)\n",
      "Epoch:  592\n",
      "Training  10 / 10  Loss:  tensor(0.8958, grad_fn=<DivBackward0>)\n",
      "Epoch:  593\n",
      "Training  10 / 10  Loss:  tensor(0.8959, grad_fn=<DivBackward0>)\n",
      "Epoch:  594\n",
      "Training  10 / 10  Loss:  tensor(0.8963, grad_fn=<DivBackward0>)\n",
      "Epoch:  595\n",
      "Training  10 / 10  Loss:  tensor(0.8947, grad_fn=<DivBackward0>)\n",
      "Epoch:  596\n",
      "Training  10 / 10  Loss:  tensor(0.8968, grad_fn=<DivBackward0>)\n",
      "Epoch:  597\n",
      "Training  10 / 10  Loss:  tensor(0.8979, grad_fn=<DivBackward0>)\n",
      "Epoch:  598\n",
      "Training  10 / 10  Loss:  tensor(0.8971, grad_fn=<DivBackward0>)\n",
      "Epoch:  599\n",
      "Training  10 / 10  Loss:  tensor(0.8965, grad_fn=<DivBackward0>)\n",
      "Epoch:  600\n",
      "Training  10 / 10  Loss:  tensor(0.8955, grad_fn=<DivBackward0>)\n",
      "Epoch:  601\n",
      "Training  10 / 10  Loss:  tensor(0.8963, grad_fn=<DivBackward0>)\n",
      "Epoch:  602\n",
      "Training  10 / 10  Loss:  tensor(0.8952, grad_fn=<DivBackward0>)\n",
      "Epoch:  603\n",
      "Training  10 / 10  Loss:  tensor(0.8981, grad_fn=<DivBackward0>)\n",
      "Epoch:  604\n",
      "Training  10 / 10  Loss:  tensor(0.8971, grad_fn=<DivBackward0>)\n",
      "Epoch:  605\n",
      "Training  10 / 10  Loss:  tensor(0.8987, grad_fn=<DivBackward0>)\n",
      "Epoch:  606\n",
      "Training  10 / 10  Loss:  tensor(0.8954, grad_fn=<DivBackward0>)\n",
      "Epoch:  607\n",
      "Training  10 / 10  Loss:  tensor(0.8959, grad_fn=<DivBackward0>)\n",
      "Epoch:  608\n",
      "Training  10 / 10  Loss:  tensor(0.8961, grad_fn=<DivBackward0>)\n",
      "Epoch:  609\n",
      "Training  10 / 10  Loss:  tensor(0.8983, grad_fn=<DivBackward0>)\n",
      "Epoch:  610\n",
      "Training  10 / 10  Loss:  tensor(0.8979, grad_fn=<DivBackward0>)\n",
      "Epoch:  611\n",
      "Training  10 / 10  Loss:  tensor(0.8969, grad_fn=<DivBackward0>)\n",
      "Epoch:  612\n",
      "Training  10 / 10  Loss:  tensor(0.8971, grad_fn=<DivBackward0>)\n",
      "Epoch:  613\n",
      "Training  10 / 10  Loss:  tensor(0.8960, grad_fn=<DivBackward0>)\n",
      "Epoch:  614\n",
      "Training  10 / 10  Loss:  tensor(0.9004, grad_fn=<DivBackward0>)\n",
      "Epoch:  615\n",
      "Training  10 / 10  Loss:  tensor(0.8975, grad_fn=<DivBackward0>)\n",
      "Epoch:  616\n",
      "Training  10 / 10  Loss:  tensor(0.8964, grad_fn=<DivBackward0>)\n",
      "Epoch:  617\n",
      "Training  10 / 10  Loss:  tensor(0.8946, grad_fn=<DivBackward0>)\n",
      "Epoch:  618\n",
      "Training  10 / 10  Loss:  tensor(0.8955, grad_fn=<DivBackward0>)\n",
      "Epoch:  619\n",
      "Training  10 / 10  Loss:  tensor(0.8931, grad_fn=<DivBackward0>)\n",
      "Epoch:  620\n",
      "Training  10 / 10  Loss:  tensor(0.8941, grad_fn=<DivBackward0>)\n",
      "Epoch:  621\n",
      "Training  10 / 10  Loss:  tensor(0.8937, grad_fn=<DivBackward0>)\n",
      "Epoch:  622\n",
      "Training  10 / 10  Loss:  tensor(0.8951, grad_fn=<DivBackward0>)\n",
      "Epoch:  623\n",
      "Training  10 / 10  Loss:  tensor(0.8944, grad_fn=<DivBackward0>)\n",
      "Epoch:  624\n",
      "Training  10 / 10  Loss:  tensor(0.8964, grad_fn=<DivBackward0>)\n",
      "Epoch:  625\n",
      "Training  10 / 10  Loss:  tensor(0.9003, grad_fn=<DivBackward0>)\n",
      "Epoch:  626\n",
      "Training  10 / 10  Loss:  tensor(0.8967, grad_fn=<DivBackward0>)\n",
      "Epoch:  627\n",
      "Training  10 / 10  Loss:  tensor(0.8938, grad_fn=<DivBackward0>)\n",
      "Epoch:  628\n",
      "Training  10 / 10  Loss:  tensor(0.8923, grad_fn=<DivBackward0>)\n",
      "Epoch:  629\n",
      "Training  10 / 10  Loss:  tensor(0.8931, grad_fn=<DivBackward0>)\n",
      "Epoch:  630\n",
      "Training  10 / 10  Loss:  tensor(0.8955, grad_fn=<DivBackward0>)\n",
      "Epoch:  631\n",
      "Training  10 / 10  Loss:  tensor(0.8950, grad_fn=<DivBackward0>)\n",
      "Epoch:  632\n",
      "Training  10 / 10  Loss:  tensor(0.8946, grad_fn=<DivBackward0>)\n",
      "Epoch:  633\n",
      "Training  10 / 10  Loss:  tensor(0.8953, grad_fn=<DivBackward0>)\n",
      "Epoch:  634\n",
      "Training  10 / 10  Loss:  tensor(0.8963, grad_fn=<DivBackward0>)\n",
      "Epoch:  635\n",
      "Training  10 / 10  Loss:  tensor(0.8968, grad_fn=<DivBackward0>)\n",
      "Epoch:  636\n",
      "Training  10 / 10  Loss:  tensor(0.8928, grad_fn=<DivBackward0>)\n",
      "Epoch:  637\n",
      "Training  10 / 10  Loss:  tensor(0.8928, grad_fn=<DivBackward0>)\n",
      "Epoch:  638\n",
      "Training  10 / 10  Loss:  tensor(0.8930, grad_fn=<DivBackward0>)\n",
      "Epoch:  639\n",
      "Training  10 / 10  Loss:  tensor(0.8929, grad_fn=<DivBackward0>)\n",
      "Epoch:  640\n",
      "Training  10 / 10  Loss:  tensor(0.8937, grad_fn=<DivBackward0>)\n",
      "Epoch:  641\n",
      "Training  10 / 10  Loss:  tensor(0.8963, grad_fn=<DivBackward0>)\n",
      "Epoch:  642\n",
      "Training  10 / 10  Loss:  tensor(0.8939, grad_fn=<DivBackward0>)\n",
      "Epoch:  643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(0.8961, grad_fn=<DivBackward0>)\n",
      "Epoch:  644\n",
      "Training  10 / 10  Loss:  tensor(0.8930, grad_fn=<DivBackward0>)\n",
      "Epoch:  645\n",
      "Training  10 / 10  Loss:  tensor(0.8953, grad_fn=<DivBackward0>)\n",
      "Epoch:  646\n",
      "Training  10 / 10  Loss:  tensor(0.8928, grad_fn=<DivBackward0>)\n",
      "Epoch:  647\n",
      "Training  10 / 10  Loss:  tensor(0.8918, grad_fn=<DivBackward0>)\n",
      "Epoch:  648\n",
      "Training  10 / 10  Loss:  tensor(0.8926, grad_fn=<DivBackward0>)\n",
      "Epoch:  649\n",
      "Training  10 / 10  Loss:  tensor(0.8943, grad_fn=<DivBackward0>)\n",
      "Epoch:  650\n",
      "Training  10 / 10  Loss:  tensor(0.8945, grad_fn=<DivBackward0>)\n",
      "Epoch:  651\n",
      "Training  10 / 10  Loss:  tensor(0.8938, grad_fn=<DivBackward0>)\n",
      "Epoch:  652\n",
      "Training  10 / 10  Loss:  tensor(0.8941, grad_fn=<DivBackward0>)\n",
      "Epoch:  653\n",
      "Training  10 / 10  Loss:  tensor(0.8940, grad_fn=<DivBackward0>)\n",
      "Epoch:  654\n",
      "Training  10 / 10  Loss:  tensor(0.8951, grad_fn=<DivBackward0>)\n",
      "Epoch:  655\n",
      "Training  10 / 10  Loss:  tensor(0.8947, grad_fn=<DivBackward0>)\n",
      "Epoch:  656\n",
      "Training  10 / 10  Loss:  tensor(0.8943, grad_fn=<DivBackward0>)\n",
      "Epoch:  657\n",
      "Training  10 / 10  Loss:  tensor(0.8927, grad_fn=<DivBackward0>)\n",
      "Epoch:  658\n",
      "Training  10 / 10  Loss:  tensor(0.8934, grad_fn=<DivBackward0>)\n",
      "Epoch:  659\n",
      "Training  10 / 10  Loss:  tensor(0.8935, grad_fn=<DivBackward0>)\n",
      "Epoch:  660\n",
      "Training  10 / 10  Loss:  tensor(0.8935, grad_fn=<DivBackward0>)\n",
      "Epoch:  661\n",
      "Training  10 / 10  Loss:  tensor(0.8926, grad_fn=<DivBackward0>)\n",
      "Epoch:  662\n",
      "Training  10 / 10  Loss:  tensor(0.8935, grad_fn=<DivBackward0>)\n",
      "Epoch:  663\n",
      "Training  10 / 10  Loss:  tensor(0.8934, grad_fn=<DivBackward0>)\n",
      "Epoch:  664\n",
      "Training  10 / 10  Loss:  tensor(0.8930, grad_fn=<DivBackward0>)\n",
      "Epoch:  665\n",
      "Training  10 / 10  Loss:  tensor(0.8926, grad_fn=<DivBackward0>)\n",
      "Epoch:  666\n",
      "Training  10 / 10  Loss:  tensor(0.8924, grad_fn=<DivBackward0>)\n",
      "Epoch:  667\n",
      "Training  10 / 10  Loss:  tensor(0.8917, grad_fn=<DivBackward0>)\n",
      "Epoch:  668\n",
      "Training  10 / 10  Loss:  tensor(0.8926, grad_fn=<DivBackward0>)\n",
      "Epoch:  669\n",
      "Training  10 / 10  Loss:  tensor(0.8946, grad_fn=<DivBackward0>)\n",
      "Epoch:  670\n",
      "Training  10 / 10  Loss:  tensor(0.8917, grad_fn=<DivBackward0>)\n",
      "Epoch:  671\n",
      "Training  10 / 10  Loss:  tensor(0.8920, grad_fn=<DivBackward0>)\n",
      "Epoch:  672\n",
      "Training  10 / 10  Loss:  tensor(0.8926, grad_fn=<DivBackward0>)\n",
      "Epoch:  673\n",
      "Training  10 / 10  Loss:  tensor(0.8912, grad_fn=<DivBackward0>)\n",
      "Epoch:  674\n",
      "Training  10 / 10  Loss:  tensor(0.8925, grad_fn=<DivBackward0>)\n",
      "Epoch:  675\n",
      "Training  10 / 10  Loss:  tensor(0.8914, grad_fn=<DivBackward0>)\n",
      "Epoch:  676\n",
      "Training  10 / 10  Loss:  tensor(0.8917, grad_fn=<DivBackward0>)\n",
      "Epoch:  677\n",
      "Training  10 / 10  Loss:  tensor(0.8922, grad_fn=<DivBackward0>)\n",
      "Epoch:  678\n",
      "Training  10 / 10  Loss:  tensor(0.8916, grad_fn=<DivBackward0>)\n",
      "Epoch:  679\n",
      "Training  10 / 10  Loss:  tensor(0.8932, grad_fn=<DivBackward0>)\n",
      "Epoch:  680\n",
      "Training  10 / 10  Loss:  tensor(0.8921, grad_fn=<DivBackward0>)\n",
      "Epoch:  681\n",
      "Training  10 / 10  Loss:  tensor(0.8939, grad_fn=<DivBackward0>)\n",
      "Epoch:  682\n",
      "Training  10 / 10  Loss:  tensor(0.8941, grad_fn=<DivBackward0>)\n",
      "Epoch:  683\n",
      "Training  10 / 10  Loss:  tensor(0.8941, grad_fn=<DivBackward0>)\n",
      "Epoch:  684\n",
      "Training  10 / 10  Loss:  tensor(0.8936, grad_fn=<DivBackward0>)\n",
      "Epoch:  685\n",
      "Training  10 / 10  Loss:  tensor(0.8929, grad_fn=<DivBackward0>)\n",
      "Epoch:  686\n",
      "Training  10 / 10  Loss:  tensor(0.8925, grad_fn=<DivBackward0>)\n",
      "Epoch:  687\n",
      "Training  10 / 10  Loss:  tensor(0.8928, grad_fn=<DivBackward0>)\n",
      "Epoch:  688\n",
      "Training  10 / 10  Loss:  tensor(0.8911, grad_fn=<DivBackward0>)\n",
      "Epoch:  689\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  690\n",
      "Training  10 / 10  Loss:  tensor(0.8906, grad_fn=<DivBackward0>)\n",
      "Epoch:  691\n",
      "Training  10 / 10  Loss:  tensor(0.8929, grad_fn=<DivBackward0>)\n",
      "Epoch:  692\n",
      "Training  10 / 10  Loss:  tensor(0.8927, grad_fn=<DivBackward0>)\n",
      "Epoch:  693\n",
      "Training  10 / 10  Loss:  tensor(0.8947, grad_fn=<DivBackward0>)\n",
      "Epoch:  694\n",
      "Training  10 / 10  Loss:  tensor(0.8931, grad_fn=<DivBackward0>)\n",
      "Epoch:  695\n",
      "Training  10 / 10  Loss:  tensor(0.8927, grad_fn=<DivBackward0>)\n",
      "Epoch:  696\n",
      "Training  10 / 10  Loss:  tensor(0.8928, grad_fn=<DivBackward0>)\n",
      "Epoch:  697\n",
      "Training  10 / 10  Loss:  tensor(0.8936, grad_fn=<DivBackward0>)\n",
      "Epoch:  698\n",
      "Training  10 / 10  Loss:  tensor(0.8946, grad_fn=<DivBackward0>)\n",
      "Epoch:  699\n",
      "Training  10 / 10  Loss:  tensor(0.8931, grad_fn=<DivBackward0>)\n",
      "Epoch:  700\n",
      "Training  10 / 10  Loss:  tensor(0.8917, grad_fn=<DivBackward0>)\n",
      "Epoch:  701\n",
      "Training  10 / 10  Loss:  tensor(0.8905, grad_fn=<DivBackward0>)\n",
      "Epoch:  702\n",
      "Training  10 / 10  Loss:  tensor(0.8907, grad_fn=<DivBackward0>)\n",
      "Epoch:  703\n",
      "Training  10 / 10  Loss:  tensor(0.8921, grad_fn=<DivBackward0>)\n",
      "Epoch:  704\n",
      "Training  10 / 10  Loss:  tensor(0.8914, grad_fn=<DivBackward0>)\n",
      "Epoch:  705\n",
      "Training  10 / 10  Loss:  tensor(0.8907, grad_fn=<DivBackward0>)\n",
      "Epoch:  706\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  707\n",
      "Training  10 / 10  Loss:  tensor(0.8924, grad_fn=<DivBackward0>)\n",
      "Epoch:  708\n",
      "Training  10 / 10  Loss:  tensor(0.8906, grad_fn=<DivBackward0>)\n",
      "Epoch:  709\n",
      "Training  10 / 10  Loss:  tensor(0.8918, grad_fn=<DivBackward0>)\n",
      "Epoch:  710\n",
      "Training  10 / 10  Loss:  tensor(0.8908, grad_fn=<DivBackward0>)\n",
      "Epoch:  711\n",
      "Training  10 / 10  Loss:  tensor(0.8918, grad_fn=<DivBackward0>)\n",
      "Epoch:  712\n",
      "Training  10 / 10  Loss:  tensor(0.8910, grad_fn=<DivBackward0>)\n",
      "Epoch:  713\n",
      "Training  10 / 10  Loss:  tensor(0.8919, grad_fn=<DivBackward0>)\n",
      "Epoch:  714\n",
      "Training  10 / 10  Loss:  tensor(0.8926, grad_fn=<DivBackward0>)\n",
      "Epoch:  715\n",
      "Training  10 / 10  Loss:  tensor(0.8919, grad_fn=<DivBackward0>)\n",
      "Epoch:  716\n",
      "Training  10 / 10  Loss:  tensor(0.8914, grad_fn=<DivBackward0>)\n",
      "Epoch:  717\n",
      "Training  10 / 10  Loss:  tensor(0.8927, grad_fn=<DivBackward0>)\n",
      "Epoch:  718\n",
      "Training  10 / 10  Loss:  tensor(0.8931, grad_fn=<DivBackward0>)\n",
      "Epoch:  719\n",
      "Training  10 / 10  Loss:  tensor(0.8929, grad_fn=<DivBackward0>)\n",
      "Epoch:  720\n",
      "Training  10 / 10  Loss:  tensor(0.8912, grad_fn=<DivBackward0>)\n",
      "Epoch:  721\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  722\n",
      "Training  10 / 10  Loss:  tensor(0.8923, grad_fn=<DivBackward0>)\n",
      "Epoch:  723\n",
      "Training  10 / 10  Loss:  tensor(0.8915, grad_fn=<DivBackward0>)\n",
      "Epoch:  724\n",
      "Training  10 / 10  Loss:  tensor(0.8905, grad_fn=<DivBackward0>)\n",
      "Epoch:  725\n",
      "Training  10 / 10  Loss:  tensor(0.8902, grad_fn=<DivBackward0>)\n",
      "Epoch:  726\n",
      "Training  10 / 10  Loss:  tensor(0.8904, grad_fn=<DivBackward0>)\n",
      "Epoch:  727\n",
      "Training  10 / 10  Loss:  tensor(0.8904, grad_fn=<DivBackward0>)\n",
      "Epoch:  728\n",
      "Training  10 / 10  Loss:  tensor(0.8907, grad_fn=<DivBackward0>)\n",
      "Epoch:  729\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  730\n",
      "Training  10 / 10  Loss:  tensor(0.8905, grad_fn=<DivBackward0>)\n",
      "Epoch:  731\n",
      "Training  10 / 10  Loss:  tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "Epoch:  732\n",
      "Training  10 / 10  Loss:  tensor(0.8898, grad_fn=<DivBackward0>)\n",
      "Epoch:  733\n",
      "Training  10 / 10  Loss:  tensor(0.8899, grad_fn=<DivBackward0>)\n",
      "Epoch:  734\n",
      "Training  10 / 10  Loss:  tensor(0.8906, grad_fn=<DivBackward0>)\n",
      "Epoch:  735\n",
      "Training  10 / 10  Loss:  tensor(0.8906, grad_fn=<DivBackward0>)\n",
      "Epoch:  736\n",
      "Training  10 / 10  Loss:  tensor(0.8914, grad_fn=<DivBackward0>)\n",
      "Epoch:  737\n",
      "Training  10 / 10  Loss:  tensor(0.8912, grad_fn=<DivBackward0>)\n",
      "Epoch:  738\n",
      "Training  10 / 10  Loss:  tensor(0.8901, grad_fn=<DivBackward0>)\n",
      "Epoch:  739\n",
      "Training  10 / 10  Loss:  tensor(0.8911, grad_fn=<DivBackward0>)\n",
      "Epoch:  740\n",
      "Training  10 / 10  Loss:  tensor(0.8904, grad_fn=<DivBackward0>)\n",
      "Epoch:  741\n",
      "Training  10 / 10  Loss:  tensor(0.8910, grad_fn=<DivBackward0>)\n",
      "Epoch:  742\n",
      "Training  10 / 10  Loss:  tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "Epoch:  743\n",
      "Training  10 / 10  Loss:  tensor(0.8912, grad_fn=<DivBackward0>)\n",
      "Epoch:  744\n",
      "Training  10 / 10  Loss:  tensor(0.8902, grad_fn=<DivBackward0>)\n",
      "Epoch:  745\n",
      "Training  10 / 10  Loss:  tensor(0.8902, grad_fn=<DivBackward0>)\n",
      "Epoch:  746\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  747\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  748\n",
      "Training  10 / 10  Loss:  tensor(0.8908, grad_fn=<DivBackward0>)\n",
      "Epoch:  749\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(0.8902, grad_fn=<DivBackward0>)\n",
      "Epoch:  751\n",
      "Training  10 / 10  Loss:  tensor(0.8904, grad_fn=<DivBackward0>)\n",
      "Epoch:  752\n",
      "Training  10 / 10  Loss:  tensor(0.8897, grad_fn=<DivBackward0>)\n",
      "Epoch:  753\n",
      "Training  10 / 10  Loss:  tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "Epoch:  754\n",
      "Training  10 / 10  Loss:  tensor(0.8921, grad_fn=<DivBackward0>)\n",
      "Epoch:  755\n",
      "Training  10 / 10  Loss:  tensor(0.8905, grad_fn=<DivBackward0>)\n",
      "Epoch:  756\n",
      "Training  10 / 10  Loss:  tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "Epoch:  757\n",
      "Training  10 / 10  Loss:  tensor(0.8900, grad_fn=<DivBackward0>)\n",
      "Epoch:  758\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  759\n",
      "Training  10 / 10  Loss:  tensor(0.8912, grad_fn=<DivBackward0>)\n",
      "Epoch:  760\n",
      "Training  10 / 10  Loss:  tensor(0.8909, grad_fn=<DivBackward0>)\n",
      "Epoch:  761\n",
      "Training  10 / 10  Loss:  tensor(0.8922, grad_fn=<DivBackward0>)\n",
      "Epoch:  762\n",
      "Training  10 / 10  Loss:  tensor(0.8922, grad_fn=<DivBackward0>)\n",
      "Epoch:  763\n",
      "Training  10 / 10  Loss:  tensor(0.8905, grad_fn=<DivBackward0>)\n",
      "Epoch:  764\n",
      "Training  10 / 10  Loss:  tensor(0.8907, grad_fn=<DivBackward0>)\n",
      "Epoch:  765\n",
      "Training  10 / 10  Loss:  tensor(0.8907, grad_fn=<DivBackward0>)\n",
      "Epoch:  766\n",
      "Training  10 / 10  Loss:  tensor(0.8905, grad_fn=<DivBackward0>)\n",
      "Epoch:  767\n",
      "Training  10 / 10  Loss:  tensor(0.8901, grad_fn=<DivBackward0>)\n",
      "Epoch:  768\n",
      "Training  10 / 10  Loss:  tensor(0.8901, grad_fn=<DivBackward0>)\n",
      "Epoch:  769\n",
      "Training  10 / 10  Loss:  tensor(0.8900, grad_fn=<DivBackward0>)\n",
      "Epoch:  770\n",
      "Training  10 / 10  Loss:  tensor(0.8898, grad_fn=<DivBackward0>)\n",
      "Epoch:  771\n",
      "Training  10 / 10  Loss:  tensor(0.8898, grad_fn=<DivBackward0>)\n",
      "Epoch:  772\n",
      "Training  10 / 10  Loss:  tensor(0.8896, grad_fn=<DivBackward0>)\n",
      "Epoch:  773\n",
      "Training  10 / 10  Loss:  tensor(0.8893, grad_fn=<DivBackward0>)\n",
      "Epoch:  774\n",
      "Training  10 / 10  Loss:  tensor(0.8898, grad_fn=<DivBackward0>)\n",
      "Epoch:  775\n",
      "Training  10 / 10  Loss:  tensor(0.8907, grad_fn=<DivBackward0>)\n",
      "Epoch:  776\n",
      "Training  10 / 10  Loss:  tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "Epoch:  777\n",
      "Training  10 / 10  Loss:  tensor(0.8900, grad_fn=<DivBackward0>)\n",
      "Epoch:  778\n",
      "Training  10 / 10  Loss:  tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "Epoch:  779\n",
      "Training  10 / 10  Loss:  tensor(0.8898, grad_fn=<DivBackward0>)\n",
      "Epoch:  780\n",
      "Training  10 / 10  Loss:  tensor(0.8905, grad_fn=<DivBackward0>)\n",
      "Epoch:  781\n",
      "Training  10 / 10  Loss:  tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "Epoch:  782\n",
      "Training  10 / 10  Loss:  tensor(0.8895, grad_fn=<DivBackward0>)\n",
      "Epoch:  783\n",
      "Training  10 / 10  Loss:  tensor(0.8893, grad_fn=<DivBackward0>)\n",
      "Epoch:  784\n",
      "Training  10 / 10  Loss:  tensor(0.8895, grad_fn=<DivBackward0>)\n",
      "Epoch:  785\n",
      "Training  10 / 10  Loss:  tensor(0.8896, grad_fn=<DivBackward0>)\n",
      "Epoch:  786\n",
      "Training  10 / 10  Loss:  tensor(0.8900, grad_fn=<DivBackward0>)\n",
      "Epoch:  787\n",
      "Training  10 / 10  Loss:  tensor(0.8898, grad_fn=<DivBackward0>)\n",
      "Epoch:  788\n",
      "Training  10 / 10  Loss:  tensor(0.8897, grad_fn=<DivBackward0>)\n",
      "Epoch:  789\n",
      "Training  10 / 10  Loss:  tensor(0.8893, grad_fn=<DivBackward0>)\n",
      "Epoch:  790\n",
      "Training  10 / 10  Loss:  tensor(0.8890, grad_fn=<DivBackward0>)\n",
      "Epoch:  791\n",
      "Training  10 / 10  Loss:  tensor(0.8889, grad_fn=<DivBackward0>)\n",
      "Epoch:  792\n",
      "Training  10 / 10  Loss:  tensor(0.8893, grad_fn=<DivBackward0>)\n",
      "Epoch:  793\n",
      "Training  10 / 10  Loss:  tensor(0.8895, grad_fn=<DivBackward0>)\n",
      "Epoch:  794\n",
      "Training  10 / 10  Loss:  tensor(0.8892, grad_fn=<DivBackward0>)\n",
      "Epoch:  795\n",
      "Training  10 / 10  Loss:  tensor(0.8891, grad_fn=<DivBackward0>)\n",
      "Epoch:  796\n",
      "Training  10 / 10  Loss:  tensor(0.8892, grad_fn=<DivBackward0>)\n",
      "Epoch:  797\n",
      "Training  10 / 10  Loss:  tensor(0.8891, grad_fn=<DivBackward0>)\n",
      "Epoch:  798\n",
      "Training  10 / 10  Loss:  tensor(0.8888, grad_fn=<DivBackward0>)\n",
      "Epoch:  799\n",
      "Training  10 / 10  Loss:  tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "Epoch:  800\n",
      "Training  10 / 10  Loss:  tensor(0.8888, grad_fn=<DivBackward0>)\n",
      "Epoch:  801\n",
      "Training  10 / 10  Loss:  tensor(0.8889, grad_fn=<DivBackward0>)\n",
      "Epoch:  802\n",
      "Training  10 / 10  Loss:  tensor(0.8888, grad_fn=<DivBackward0>)\n",
      "Epoch:  803\n",
      "Training  10 / 10  Loss:  tensor(0.8894, grad_fn=<DivBackward0>)\n",
      "Epoch:  804\n",
      "Training  10 / 10  Loss:  tensor(0.8891, grad_fn=<DivBackward0>)\n",
      "Epoch:  805\n",
      "Training  10 / 10  Loss:  tensor(0.8888, grad_fn=<DivBackward0>)\n",
      "Epoch:  806\n",
      "Training  10 / 10  Loss:  tensor(0.8889, grad_fn=<DivBackward0>)\n",
      "Epoch:  807\n",
      "Training  10 / 10  Loss:  tensor(0.8885, grad_fn=<DivBackward0>)\n",
      "Epoch:  808\n",
      "Training  10 / 10  Loss:  tensor(0.8885, grad_fn=<DivBackward0>)\n",
      "Epoch:  809\n",
      "Training  10 / 10  Loss:  tensor(0.8889, grad_fn=<DivBackward0>)\n",
      "Epoch:  810\n",
      "Training  10 / 10  Loss:  tensor(0.8888, grad_fn=<DivBackward0>)\n",
      "Epoch:  811\n",
      "Training  10 / 10  Loss:  tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "Epoch:  812\n",
      "Training  10 / 10  Loss:  tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "Epoch:  813\n",
      "Training  10 / 10  Loss:  tensor(0.8883, grad_fn=<DivBackward0>)\n",
      "Epoch:  814\n",
      "Training  10 / 10  Loss:  tensor(0.8882, grad_fn=<DivBackward0>)\n",
      "Epoch:  815\n",
      "Training  10 / 10  Loss:  tensor(0.8883, grad_fn=<DivBackward0>)\n",
      "Epoch:  816\n",
      "Training  10 / 10  Loss:  tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "Epoch:  817\n",
      "Training  10 / 10  Loss:  tensor(0.8885, grad_fn=<DivBackward0>)\n",
      "Epoch:  818\n",
      "Training  10 / 10  Loss:  tensor(0.8884, grad_fn=<DivBackward0>)\n",
      "Epoch:  819\n",
      "Training  10 / 10  Loss:  tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "Epoch:  820\n",
      "Training  10 / 10  Loss:  tensor(0.8884, grad_fn=<DivBackward0>)\n",
      "Epoch:  821\n",
      "Training  10 / 10  Loss:  tensor(0.8880, grad_fn=<DivBackward0>)\n",
      "Epoch:  822\n",
      "Training  10 / 10  Loss:  tensor(0.8881, grad_fn=<DivBackward0>)\n",
      "Epoch:  823\n",
      "Training  10 / 10  Loss:  tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "Epoch:  824\n",
      "Training  10 / 10  Loss:  tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "Epoch:  825\n",
      "Training  10 / 10  Loss:  tensor(0.8884, grad_fn=<DivBackward0>)\n",
      "Epoch:  826\n",
      "Training  10 / 10  Loss:  tensor(0.8882, grad_fn=<DivBackward0>)\n",
      "Epoch:  827\n",
      "Training  10 / 10  Loss:  tensor(0.8880, grad_fn=<DivBackward0>)\n",
      "Epoch:  828\n",
      "Training  10 / 10  Loss:  tensor(0.8881, grad_fn=<DivBackward0>)\n",
      "Epoch:  829\n",
      "Training  10 / 10  Loss:  tensor(0.8881, grad_fn=<DivBackward0>)\n",
      "Epoch:  830\n",
      "Training  10 / 10  Loss:  tensor(0.8881, grad_fn=<DivBackward0>)\n",
      "Epoch:  831\n",
      "Training  10 / 10  Loss:  tensor(0.8879, grad_fn=<DivBackward0>)\n",
      "Epoch:  832\n",
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  833\n",
      "Training  10 / 10  Loss:  tensor(0.8877, grad_fn=<DivBackward0>)\n",
      "Epoch:  834\n",
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  835\n",
      "Training  10 / 10  Loss:  tensor(0.8879, grad_fn=<DivBackward0>)\n",
      "Epoch:  836\n",
      "Training  10 / 10  Loss:  tensor(0.8881, grad_fn=<DivBackward0>)\n",
      "Epoch:  837\n",
      "Training  10 / 10  Loss:  tensor(0.8882, grad_fn=<DivBackward0>)\n",
      "Epoch:  838\n",
      "Training  10 / 10  Loss:  tensor(0.8889, grad_fn=<DivBackward0>)\n",
      "Epoch:  839\n",
      "Training  10 / 10  Loss:  tensor(0.8882, grad_fn=<DivBackward0>)\n",
      "Epoch:  840\n",
      "Training  10 / 10  Loss:  tensor(0.8881, grad_fn=<DivBackward0>)\n",
      "Epoch:  841\n",
      "Training  10 / 10  Loss:  tensor(0.8879, grad_fn=<DivBackward0>)\n",
      "Epoch:  842\n",
      "Training  10 / 10  Loss:  tensor(0.8880, grad_fn=<DivBackward0>)\n",
      "Epoch:  843\n",
      "Training  10 / 10  Loss:  tensor(0.8879, grad_fn=<DivBackward0>)\n",
      "Epoch:  844\n",
      "Training  10 / 10  Loss:  tensor(0.8879, grad_fn=<DivBackward0>)\n",
      "Epoch:  845\n",
      "Training  10 / 10  Loss:  tensor(0.8880, grad_fn=<DivBackward0>)\n",
      "Epoch:  846\n",
      "Training  10 / 10  Loss:  tensor(0.8883, grad_fn=<DivBackward0>)\n",
      "Epoch:  847\n",
      "Training  10 / 10  Loss:  tensor(0.8880, grad_fn=<DivBackward0>)\n",
      "Epoch:  848\n",
      "Training  10 / 10  Loss:  tensor(0.8879, grad_fn=<DivBackward0>)\n",
      "Epoch:  849\n",
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  850\n",
      "Training  10 / 10  Loss:  tensor(0.8877, grad_fn=<DivBackward0>)\n",
      "Epoch:  851\n",
      "Training  10 / 10  Loss:  tensor(0.8877, grad_fn=<DivBackward0>)\n",
      "Epoch:  852\n",
      "Training  10 / 10  Loss:  tensor(0.8877, grad_fn=<DivBackward0>)\n",
      "Epoch:  853\n",
      "Training  10 / 10  Loss:  tensor(0.8877, grad_fn=<DivBackward0>)\n",
      "Epoch:  854\n",
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  855\n",
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  856\n",
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  858\n",
      "Training  10 / 10  Loss:  tensor(0.8878, grad_fn=<DivBackward0>)\n",
      "Epoch:  859\n",
      "Training  10 / 10  Loss:  tensor(0.8879, grad_fn=<DivBackward0>)\n",
      "Epoch:  860\n",
      "Training  10 / 10  Loss:  tensor(0.8877, grad_fn=<DivBackward0>)\n",
      "Epoch:  861\n",
      "Training  10 / 10  Loss:  tensor(0.8876, grad_fn=<DivBackward0>)\n",
      "Epoch:  862\n",
      "Training  10 / 10  Loss:  tensor(0.8876, grad_fn=<DivBackward0>)\n",
      "Epoch:  863\n",
      "Training  10 / 10  Loss:  tensor(0.8876, grad_fn=<DivBackward0>)\n",
      "Epoch:  864\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  865\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  866\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  867\n",
      "Training  10 / 10  Loss:  tensor(0.8876, grad_fn=<DivBackward0>)\n",
      "Epoch:  868\n",
      "Training  10 / 10  Loss:  tensor(0.8876, grad_fn=<DivBackward0>)\n",
      "Epoch:  869\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  870\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  871\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  872\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  873\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  874\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  875\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  876\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  877\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  878\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  879\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  880\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  881\n",
      "Training  10 / 10  Loss:  tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "Epoch:  882\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  883\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  884\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  885\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  886\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  887\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  888\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  889\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  890\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  891\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  892\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  893\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  894\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  895\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  896\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  897\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  898\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  899\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  900\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  901\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  902\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  903\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  904\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  905\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  906\n",
      "Training  10 / 10  Loss:  tensor(0.8874, grad_fn=<DivBackward0>)\n",
      "Epoch:  907\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  908\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  909\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  910\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  911\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  912\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  913\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  914\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  915\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  916\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  917\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  918\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  919\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  920\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  921\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  922\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  923\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  924\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  925\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  926\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  927\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  928\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  929\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  930\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  931\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  932\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  933\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  934\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  935\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  936\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  937\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  938\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  939\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  940\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  941\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  942\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  943\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  944\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  945\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  946\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  947\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  948\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  949\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  950\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  951\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  952\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  953\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  954\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  955\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  956\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  957\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  958\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  959\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  960\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  961\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  962\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  963\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  965\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  966\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  967\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  968\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  969\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  970\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  971\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  972\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  973\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  974\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  975\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  976\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  977\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  978\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  979\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  980\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  981\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  982\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  983\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  984\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  985\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  986\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  987\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  988\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  989\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  990\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  991\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  992\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  993\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  994\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  995\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  996\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  997\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  998\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "Epoch:  999\n",
      "Training  10 / 10  Loss:  tensor(0.8873, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "epochs = 1000\n",
    "\n",
    "for epoch_counter in range(epochs):\n",
    "        loss_sum = 0\n",
    "        counter = 0\n",
    "        print('Epoch: ', epoch_counter)\n",
    "        for xs, ys in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            exif_pred = model(xs)\n",
    "            loss = loss_function(exif_pred, ys)\n",
    "            loss_sum += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            counter += 1\n",
    "            \n",
    "            if counter % 1 == 0:\n",
    "            \tprint(\"Training \",counter * batch_size,\"/\", training_images_count, \" Loss: \", loss_sum / (batch_size))\n",
    "            \tloss_sum = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
