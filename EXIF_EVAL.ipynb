{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two layer MLP to get the consistency score for each pair of patches\n",
    "class EVAL(nn.Module):\n",
    "    def __init__(self, exif_model, exif_dim, middle_dim):\n",
    "        super(EVAL, self).__init__()\n",
    "        self.exif_model = exif_model\n",
    "        self.layer1 = nn.Linear(exif_dim, middle_dim)\n",
    "        self.layer2 = nn.Linear(middle_dim, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = self.exif_model(x1, x2)        \n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXIF(nn.Module):\n",
    "    def __init__(self, encoder, n_features, projection_dim):\n",
    "        super(EXIF, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(2 * self.n_features, self.n_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_features, 2 * projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * projection_dim, projection_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        h_i = self.encoder(x_i)\n",
    "        h_j = self.encoder(x_j)\n",
    "\n",
    "        c = torch.cat((h_i, h_j), 1)\n",
    "        z = self.projector(c)\n",
    "        return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use number of different bit in mask to represent score\n",
    "def getConsistencyScore(p1, p2):\n",
    "    return sum(sum(sum(abs(p1-p2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVALDATASET(Dataset):\n",
    "    def __init__(self, root_dir, size=20):\n",
    "        self.root_dir = root_dir\n",
    "        self.size = size\n",
    "        self.file_names = [name for name in os.listdir(self.root_dir+'/images')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_name = self.file_names[idx].split('.')[0]\n",
    "\n",
    "        image = Image.open(os.path.join(self.root_dir+'/images/'+file_name+\".jpg\"))\n",
    "        mask = Image.open(os.path.join(self.root_dir+'/masks/'+file_name+\".png\"))\n",
    "        \n",
    "        \n",
    "        image = torchvision.transforms.ToTensor()(image)\n",
    "        mask = torchvision.transforms.ToTensor()(mask)\n",
    "                \n",
    "        size = self.size\n",
    "        height = image.shape[1]\n",
    "        width = image.shape[2]\n",
    "        \n",
    "        # random pick two left top corner\n",
    "        x1 = random.randint(0, height-size)\n",
    "        y1 = random.randint(0, width-size)\n",
    "        p1 = image[:3, x1:x1+size, y1:y1+size]\n",
    "        m1 = mask[:, x1:x1+size, y1:y1+size]\n",
    "        \n",
    "        x2 = random.randint(0, height-size)\n",
    "        y2 = random.randint(0, width-size)\n",
    "        p2 = image[:3, x2:x2+size, y2:y2+size]\n",
    "        m2 = mask[:, x2:x2+size, y2:y2+size]\n",
    "        \n",
    "        sample = {'p1': p1, 'p2':p2, 'score': getConsistencyScore(m1, m2)}\n",
    "#         return p1, p2, getConsistencyScore(m1, m2)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare one patch with all other patches\n",
    "def evaluatePatch(p1, image, size, eval_model):\n",
    "    score = 0\n",
    "    height = image.shape[1]\n",
    "    width = image.shape[2]\n",
    "    for i in range(height // size):\n",
    "        for j in range(width // size):\n",
    "            x = size * i\n",
    "            y = size * j\n",
    "            xs = min(x + size, height)\n",
    "            ys = min(y + size, width)\n",
    "            p2 = image[:, x:xs, y:ys]\n",
    "            \n",
    "            p1 = p1.view(1, 3, size, size)\n",
    "            p2 = p2.view(1, 3, size, size)\n",
    "#             print(p1.shape)\n",
    "#             print(p2.shape)\n",
    "            score += eval_model(p1, p2)\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate consistency map\n",
    "def getConsistencyMap(image, size, eval_model):\n",
    "    consistency_map = []\n",
    "    height = image.shape[1]\n",
    "    width = image.shape[2]\n",
    "    print(height, width)\n",
    "    for i in range(height // size):\n",
    "        row = []\n",
    "        for j in range(width // size):\n",
    "            x = size * i\n",
    "            y = size * j\n",
    "            xs = min(x + size, height)\n",
    "            ys = min(y + size, width)\n",
    "            p = image[:, x:xs, y:ys]\n",
    "            row.append(evaluatePatch(p, image, size, eval_model).item())\n",
    "            print('add',i,j)\n",
    "        consistency_map.append(row)\n",
    "    return consistency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether an image has been sliced\n",
    "def ifSliced(consistency_map, threshold):\n",
    "    return sum(sum(consistency_map)) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images_a = []\n",
    "    images_b = []\n",
    "    labels = []\n",
    "    for i in batch:\n",
    "        images_a.append(i['p1'])\n",
    "        images_b.append(i['p2'])\n",
    "        labels.append(i['score'])\n",
    "    return (torch.stack(images_a), torch.stack(images_b)), torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 16024.634765625\n",
      "loss 21322.63671875\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 20, 20] at entry 0 and [1, 20, 20] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e9e7764547b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e9e7764547b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(eval_model, size, loss_function)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sim2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sim2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sim2/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-6e1c0b5c1aca>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimages_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 20, 20] at entry 0 and [1, 20, 20] at entry 3"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import random\n",
    "\n",
    "def train(eval_model, size, loss_function):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for ((p1, p2), score)  in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = eval_model(p1, p2) # evaluation model generates a consistency score of a pair of patch\n",
    "        score = score.view(len(p1[:, 0, 0]), 1)\n",
    "#         print(output.shape)\n",
    "#         print(score.shape)\n",
    "        \n",
    "        loss = loss_function(output, score)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        print('loss', loss.item())\n",
    "\n",
    "train_dataset = EVALDATASET('dataset/label_in_wild', 20)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=10,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "    \n",
    "encoder = torchvision.models.resnet50(pretrained=False)\n",
    "n_features = 1000\n",
    "exif_model = EXIF(encoder, n_features, 8)\n",
    "exif_dim = 8\n",
    "middle_dim = 3\n",
    "size = 20\n",
    "optimizer = torch.optim.Adam(exif_model.parameters(), lr=1e-4)\n",
    "\n",
    "eval_model = EVAL(exif_model, exif_dim, middle_dim)\n",
    "loss_function = nn.MSELoss(reduction='mean')\n",
    "\n",
    "train(eval_model, size, loss_function)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 1600\n",
      "add 0 0\n",
      "add 0 1\n",
      "[[-0.6183071136474609, -0.6184686422348022]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating\n",
    "size = 600\n",
    "threshold = 100\n",
    "eval_model.eval()\n",
    "def evaluate(eval_model, image, size, threshold):\n",
    "    consistency_map = getConsistencyMap(image, size, eval_model)\n",
    "#     if_sliced = ifSliced(consistency_map, threshold)\n",
    "    \n",
    "    return consistency_map\n",
    "    \n",
    "image = Image.open(os.path.join('dataset/label_in_wild/images/im12_edit1.jpg'))\n",
    "image = torchvision.transforms.ToTensor()(image)\n",
    "\n",
    "print(evaluate(eval_model, image, size, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
